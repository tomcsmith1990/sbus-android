\documentclass[12pt,twoside,notitlepage]{report}

\usepackage{a4}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage[chapter]{algorithm}
\usepackage{algpseudocode}

% create extra algorithm commands
\algnewcommand\algorithmiccontinue{\textbf{continue}}
\algnewcommand\Continue{\State\algorithmiccontinue}

\algnewcommand\algorithmicbreak{\textbf{break}}
\algnewcommand\Break{\State\algorithmicbreak}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
    pdftitle=Dissertation,
    pdfauthor={Tom Smith},
    pdfsubject={Computer Science},
}

% set tab size for lstlisting
\lstset{tabsize=4,basicstyle=\ttfamily}

\input{epsf}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf Thomas Smith}

\vspace*{60mm}
\begin{center}
\Huge
{\bf Automatic Configuration in Mobile Environments} \\
\vspace*{5mm}
Computer Science Tripos Part II \\
\vspace*{5mm}
Sidney Sussex College \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Thomas Smith	\\
College:            & \bf Sidney Sussex College	\\
Project Title:      & \bf Automatic Configuration \\ &\bf in Mobile Environments	\\
Examination:        & \bf Computer Science Tripos Part II, 2013 	\\
Word Count:         & \bf 10896\footnotemark[1] \\
Project Originator: & Dr Jatinder Singh		\\
Supervisor:         & Dr Jatinder Singh		\\ 
\end{tabular}
}

\footnotetext[1]{This word count was computed by {\tt detex -e `appendix,figure,algorithm' diss.tex | tr -cd `0-9A-Za-z $\tt\backslash$n' | wc -w}}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The aim of the project was to investigate how applications can respond to changes in context, primarily those arising as a result of mobility. 
This requires some system which allows applications on a mobile device to communicate with those in different environments. 
There must be some mechanism for detecting context changes. 
Applications need to be able to discover and connect to relevant peers. 
Finally, applications should be able to make meaningful use of data which is slightly different to their expectations. 
This grants applications the ability to interact with a wider range of peers, allowing greater adaptability to different environments.

\section*{Work Completed}

A messaging middleware was modified to allow searching for peers by aspects of their schema, offering exact matches based on names and types, or looser matches which only consider types.
This middleware was ported for use on Android, allowing applications on a mobile device to communicate with other systems using the same middleware. 
An application was created to monitor changes in context through various sensors on the device. 
Upon detection of a change in context, this application informs other applications of the change or automatically reconfigures their connections to connect them to new peers, based on policies the appplications define.

\section*{Special Difficulties}

The main difficulty in the project came from porting the messaging middleware to Android. The Android NDK\footnote{\url{http://developer.android.com/tools/sdk/ndk/index.html}} toolkit was used in order to compile the C++ library for use on Android, however missing standard libraries and poor documentation made this task more difficult than expected.
 
\newpage
\section*{Declaration}

I Thomas Smith of Sidney Sussex College, being a candidate for Part II of the Computer
Science Tripos, hereby declare that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose. 
I give permission for my dissertation to be made available in the archive area of the Laboratory's website.

\bigskip
\bigskip
\leftline{Signed: }

\bigskip
\bigskip
\leftline{Date: }

\cleardoublepage

\tableofcontents

\listoffigures

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}

% Intro + Preparation ~ 1400 words
\chapter{Introduction}

% summarise
While many applications exist for mobile devices, few applications themselves are truly mobile. 
The user may happen to be mobile, however applications do not adapt to different environments. 
Applications may typically use data from predetermined central servers, failing to exploit local data which may be available in the environment. 
This project addresses the issues faced in creating applications which can change to suit different environments. 
The project demonstrates how applications can perceive an environmental change, how they can find relevant local data in the environment, and how they can achieve some flexibility in the data which they use. 
This allows applications to be more adaptable to their environment, and therefore offer a more relevant experience to the user. 

\section{Problem}
% what's the problem

% describe current approach
Current applications tend to be inflexible in data sources they use, often selecting from some hardcoded list of possibilities, regardless of the environment. 
While some localisation might be achieved by including parameters during connection, such as sending GPS co-ordinates, the possible parameters which will be accepted have already been fixed, restricting how well applications can adapt to their environment. 
The format of the data which is being exchanged has also been fixed, restricting applications even further.

This is not in line with the pervasive computing vision \cite{weiser1991computer}, primarily because applications do not use their current environment. 
The systems are not scalable, because applications are using global servers, and they offer no heterogeneity\cite{saha2003pervasive} in the data they use, since both application and server both use an exact format.

Applications can gain main relevant data by connection to local data source, due to the fact that these data sources reside within the same environment.
Given an application which requires the current temperature, a global server may use current GPS co-ordinates to determine the nearest weather station and return its most recent reading. 
Yet if the application were being used indoors, a value from a local temperature sensor might not only suffice, but be more appropriate. 
Perhaps this could be achieved otherwise, if the server had known about the temperature sensor, however why should the application need to connect to a global server in order to use local data? Perhaps there isn't even an Internet connection available. 
Furthermore, given that the user is mobile, the server would need to know about {\sl every} temperature sensor {\sl everywhere}. 
Local data sources allow applications to use a dynamic set of data sources, more related to their environment. 
Furthermore, applications need not predefine a list of data sources to use.

% context
Context refers to any event which the system or the user feels that it is reasonable to respond to, where any happening of interest that can be observed from within a computer is considered an event \cite[page 11]{muhl2006distributed}. 
Mobile devices constantly operate in different contexts, which dictate how the devices can interact with the environment. 
Upon a change in context, an application may move out of scope of a given data source, or the data it is providing may no longer be relevant. 
In this situation, the application must find an alternative data source which can provide similar data, or one which provides data which is more relevant to the new context. 

Once this alternate data source has been located, the application must still determine whether it provides useful data. 
Some data may be fundamental to the operation of the application, while other parts may be desired, but not crucial.  
Even once a data source is deemed useful, the application may still face the difficulty of data being presented in an unexpected format, simply because of the geographical scale and market which the application might be used in. 
Different manufacturers might present data in slightly different formats, or different countries may present data using their own language. 
These factors mean that when comparing similar pieces of data, some may be missing fields, some may contain additional fields, or fields may use different names. 
An application may also wish to specify a subscription filter about the messages it should receive, yet without knowledge of the data format, it cannot specify filters on different fields.

% example
For example, a transport application may face differences in the data format for coaches (Figure \ref{fig:coach-schema}) and for trains (Figure \ref{fig:train-schema}). 
While both offer similar data to a large extent, fields are named differently, and the train schema additionally offers a menu for an on-board shop.
Alternatively, the application might be used abroad, for example in a French coach station. The data will still be very similar, but fields names may be in French (Figure \ref{fig:french-coach-schema}).

An application developer may be able to account for some of these differences, though given the sheer volume of potential sources, the developer cannot hope to account for every possibility, meaning that the application cannot be truly adaptable. 
Even if the developer could cover every existing source, these sources are liable to change over time. 
New sources may become available and old sources may become unavailable, or sources might change how they present their data. 
A developer simply cannot cover every current and future possibility, meaning an application cannot truly adapt to any environment. 
However, by specifying how an application should react to changes in context, the application becomes decoupled from the environment, making it more robust to changes in the environment. 

\section{Project}
% what I've done

Schilit et al \cite{schilit1994context}  define a context-aware system as one which can examine the computing environment and react to changes in the environment. 
This project creates such a system, which performs automatic contextual reconfiguration for applications whenever there is a change in the environment. 
Pascoe \cite{pascoe1998adding} lists four context-aware capabilites. 
{\sl Contextual sensing} presents environmental state to the user, while {\sl contextual adaptation} allows applications to use this knowledge to adapt their behaviour to integrate with the user's environment.
{\sl Contextual resource discovery} allows applications to discover and exploit resources which exist in the same context as themselves. 
{\sl Contextual augmentation} augments reality with digital data. 
This system uses contextual resource discovery to discover potential peers available within the current context, and contextual adaptation to allow  applications to adapt to different data formats in order to communicate with these peers.

% describe context, say only focusing on mobility (Wi-Fi) for now.
M\"{u}hl represents changes in context using events, where each context change causes an event to be raised \cite[page 11]{muhl2006distributed}.
Three kinds of events are defined: physical events, timer events, and a state change in the system, all of which may warrant some kind of reconfiguration. 
In a healthcare system, a rapid accelerometer change followed by no movement may represent a physical event (a fall), in which case the application may need to send data to an emergency response team.
A timer event indicates the progression of real time, which advertisers could use to connect to advertisements for nearby restaurants at lunch time. 
Finally, there are state changes in the system, such as a change in network connection or location. 
This project primarly focuses on state changes events, particularly using a change in network connection to indicate a change in context, though in practice any event might be used.  
Composite events might even be considered to represent a context change. 
These require the detection of several different events, or one or more occurrences of a single event \cite{chakravarthy1994composite}. 

% policy engine
Applications control how their connections should be reconfigured through policies. 
A policy engine monitors the environment and upon detecting a change in context applies these policies, reconfiguring applications' connections to connect them to different data sources, where appropriate.
These policies may specify the name of data sources, or be more complex rules regarding the structure of data sources' schemas. 
Alternatively, applications may opt to be notified of a change in context, rather than have their connection reconfigured, allowing them to perform callback methods upon the change.

% repackaging
Depending on its policy, an application may be connected to a data source which does not offer the expected data schema. 
The system handles this by repackaging messages to match the application's schema. Any extraneous fields are removed, any absent fields are filled with ``empty'' values, and the policy is used to infer relationships between fields with different names. 
Once the system has repackaged a message, it passes it to the application, allowing the application to continue as if it had received a message conforming to its schema.

% subscription
The system allows an application to specify subscription filters in terms of its own schema. 
As part of the reconfiguration process the system converts this subscription to match the peer's schema, connecting the application with a working subscription in place.

% pervasive computing
The system supports many of the goals of the pervasive computing vision \cite{weiser1991computer}.
Use of local data sources allows the system to be scalable. 
Connections are created on an ad-hoc basis, based entirely upon what data sources are available within the environment. 
Repackaging messages allows applications to adapt to heterogeneous content and devices, allowing applications to operate in a multitude of environments. 
Different sources may represent data in different formats, though application policies allow the system to interpret this data in a meaningful way. 
The policy engine can perceive context, and use these perceptions in an intelligent way, by reconfiguring applications' connections according to their policies. 
While all of this is invisible to the user, applications can be configured by the user through the use of policies \cite{saha2003pervasive}.

% summarise
The system automatically reconfigures applications' connections and repackages messages to fit their schemas, granting the applications flexibility in the data sources they use.
Local data sources can be used to provide data relevant to the environment, and by repackaging these messages applications are no longer bound to exact schemas. 
Applications control how they interact with an environment through policies, while still seamlessly communicating within that environment.

\cleardoublepage

 
\chapter{Preparation}

This chapter first outlines the requirements of the system, going on to summarise related work which the project will build upon, namely the SBUS middleware \cite{ingram2009reconfigurable}, and finally describing how the system will be implemented to meet these requirements.

%TODO: Why the recursive discovery through peer inspection hasn't been implemented, and how it could be (should be somewhere else?)

\section{Requirements Analysis}

The following use cases outline the functional requirements of the system, describing how the system will operate in different circumstances.

% use cases - functional requirements
John uses a National Heritage application which can provide information about exhibits in a museum, or nearby landmarks in a city. 
While in a museum, John's phone is connected to the Wi-Fi network, and the application receives data from a source on the network, which provides a guided tour of the museum. 
When leaving the museum, John's phone disconnects from the Wi-Fi network. 
The system detects this, and connects the application to a global source via 3G, which uses his GPS co-ordinates to provide the application with information about nearby landmarks.

\begin{itemize}

\item {\bf Detect and Act upon Change in Context}

\begin{enumerate}

\item John uses the application to view information about each exhibit while walking around the museum.

\item Upon leaving the museum, his phone disconnects from the Wi-Fi network.

\item The system detects that John has left, so connects the application to a global source, which then sends information about nearby landmarks so that John can decide where to go next.

\end{enumerate}

\item {\bf Connect to Relevant Peers}

\begin{enumerate}

\item John chooses to visit a nearby art gallery, and upon entering the gallery, John connects his phone to the Wi-Fi network.

\item The system has detected this change, and searches for a source providing ``museum data''.

\item Once the system finds a source, it connects the application to it, allowing John to use the application for a guided tour of the art gallery. 

\end{enumerate}

\item {\bf Allow Communication Between Incompatible Schemas}

\begin{enumerate}

\item The art gallery is trialling a new system which includes relevant links in the data for each exhibit.  

\item John's application does not support this extra field, making the source incompatible with the application.

\item The system converts these art gallery messages to match the application data format, making the data compatible with John's application. 

\end{enumerate}

\end{itemize}

%TODO: more of this.
% non-functional requirements
Due to the ever changing nature of mobile environments, the system must also meet certain non-functional requirements to ensure that is is responsive and usable. 
The system must search for peers to connect to within a reasonable time, and any message conversion required for a given peer must incur negligible delay.
These constraints are necessary so that any communication between peers remains relevant to the user's context.

\section{SBUS}

SBUS \cite{ingram2009reconfigurable} is a messaging middleware written in C++, which can be used for stream based communication. 
The basic entity in SBUS is a {\sl component}. 
Each component consists of one or more {\sl endpoints} which can be connected, or {\sl mapped} together allowing direct communication between the two endpoints. 
Every endpoint has an associated message type, and in order for two endpoints to be mapped together, their type must match. 
An endpoint may specify a subscription filter on events it wishes to receive, in terms of its message type.
 Any connected peers will only publish events to this endpoint if the event matches the subscription.

These connections and subscriptions are dynamically reconfigurable and may be modified by any other component, allowing third party remapping to occur, subject to security policies. 
A component may issue a message to some other component, instructing it to map or unmap its endpoints, or to update its subscription filters.

Resource discovery components (RDCs) are components which act as a name service. 
An environment may contain several RDCs, or no RDCs, though typically only one exists in each environment.
Components may query an RDC for other components within the environment, using parameters including component name, connected peers, or public key. 
The RDC will return IP addresses of matching components, allowing the component to establish a direct connection to a match.

\section{System Design}

The system will be based upon the SBUS middleware, using and extending it to meet the requirements. 
Applications will implement an SBUS component, sending and receiving messages on endpoints. 

% Android NDK
SBUS will need to be ported for use on Android. 
The Android Native Development Kit\footnote{\url{http://developer.android.com/tools/sdk/ndk/index.html}} (NDK) will be used to compile SBUS for Android, and the Java Native Interface (JNI) framework will be used to allow applications to use this library. 
Since the NDK offers fairly obscure documentation with little more than trivial examples, some time will be required to understand how the build process works. 
This presents the largest unknown factor in the system, so will be the first work undertaken, to allow for modification to the project should it take more or less time than expected.

% How applications will find data sources relevant to them (constraints in terms of schema)
When components query the RDC they generally know something about the metadata of the component they want, such as its name or connected peers. 
In a mobile system, applications are unlikely to have this information available in every case, simply because there are too many possible environments. 
To allow applications to find relevant components in any environment, the system will be extended to allow RDC queries centred around data.
Applications will be able to search by the format of the data they use, connecting to any component which supports that format, without any other prior knowledge of that component.

% How applications will cope with unexpected data format - must be done at system level
SBUS endpoints can currently only communicate if they share a common message type. 
This ensures that endpoints only ever receive messages matching that type, from which values can be easily extracted since the structure of the message is known.
When searching for components by data, an application may be connected to some endpoint which contains the required message type, nested somewhere within a larger type.
% TODO: example
This means that an application may not know the structure of the message, so cannot locate a particular value within it. The application would have to perform some complex parsing of the message in order to extract the value. 
This is obviously undesirable since each environment might use a slightly different message type.
To avoid each application implementing such a parser, these differences will be handled at the system level, before the application receives the message. 
The system will reformat any incoming message to match the application message type, meaning an application will never face uncertainity in a the structure of a message. 

% How change in context will be detected
A typical smartphone offers many sensors which can be used to determine a context change. 
Android applications can register to receive intents\footnote{\url{http://developer.android.com/reference/android/content/Intent.html}} which indicate events such as a network change, or an incoming phone call. 
Furthermore, the AIRS application\footnote{\url{https://play.google.com/store/apps/details?id=com.airs}} observes sensor readings, and can output them to a remote server.
The system can use these intents, and act as a server for AIRS to obtain these sensor readings, allowing it to monitor the environment, and determine context changes. 
Upon detecting a context change, it can then remap other applications on the phone connecting them to peers suited to the environmental context. 

% How applications' connections will be reconfigured (spoke map)
In order for the system to remap applications, it will first need to know about them. 
An RDC is well suited for this, because it maintains a list of registered components, which can then be searched when a component queries the RDC. 
This list can also serve as a list of components which the system can remap.

These two aspects of detecting context change and acting as an RDC can be combined into a Phone Manager (PM) application. 
The PM will act similarly to an RDC, allowing applications on the device to register with it. 
It will detect context changes and upon a change will issue messages to registered components to instruct them to remap to other components.

% How the Phone Manager knows what to reconfigure to (policies)
The PM will remap applications upon a change in context, but must remap them to appropriate places. 
The PM cannot decide what constitutes ``appropriate'' for any given application, as such, the application must decide for itself. 
Applications will register {\sl policies} with the PM which dictate how they should be remapped upon a context change. 
This policy might be an IP address to map to, or a query which can be performed against an RDC in the environment, which will then return matching components. 
The PM will simply remap applications as specified by their policies.

% Security
This project will not focus on security. 
A secure version of SBUS which uses OpenSSL to encrypt communications between components exists, and all of the work undertaken in this project could be applied to that version of SBUS.

\cleardoublepage


% ~ 4000 words
\chapter{Implementation}

This chapter consists of three main parts. 
The first shows how applications can find data sources which offer the data they require, and how they can use these sources despite differences in schemas. 
The second part explains how the SBUS library was ported to Android, and how it can be used by application developers. 
Finally, this is all brought together with the Phone Manager which is responsible for detecting changes in context and reconfiguring applications' connections upon a change.

\section{Flexible Schema Matching}

Flexible schema matching refers to components finding peers based on aspects of their schema, and how they can communicate with these peers if their schemas are not identical. 

% syntax
The most natural way for a component to query an RDC with data constraints seemed to be to extend the current query mechanism. 
When searching, a component may be interested in an exact match, where both names and types match, or a weaker match where only types must match. 
Since a component only knows about its own schema in advance, these queries must be specified in terms of its own schema.
Two new query parameters have been introduced to allow this functionality. 
An exact match is specified by {\tt +H}, representing a schema {\sl has} an item, and a type match is specified by {\tt +S}, representing a schema has a {\sl similar} item.
Each of these is used with the name of a single item in the component's schema.
If this item is a single field, it refers only to that field, whereas if it is a composite item consisting of multiple fields, the query applies recursively to the entire item. 
Any item which does not exist in the schema is ignored. 
Table \ref{tab:example_schema_syntax} demonstrates some possible queries based on the schema in Figure \ref{fig:locationschema}.

\begin{figure}[tbh]
\begin{lstlisting}
@location
{
	gps
	{
		dbl longitude
		dbl latitude
		int altitude
	}
	txt city
}
\end{lstlisting}
\caption{Location Schema}
\label{fig:locationschema}
\end{figure}

\begin{table}[tbh]
\centering

\begin{tabular}{l l}
\hline\hline
Query & Match \\
\hline

{\tt +Scity}		& {\tt city} field type ({\tt txt}) \\
{\tt +Hlongitude}	& {\tt longitude} field exactly \\

{\tt +Sgps}			& {\tt gps} item type \\
{\tt +Hgps}			& {\tt gps} item exactly \\

{\tt +Hcountry}		& {\tt <ignored>} \\
{\tt +Hlocation}	& {\tt location} schema exactly \\

{\tt +Sgps+Hlongitude}	& {\tt gps} item type AND {\tt longitude} field exactly \\

\hline
\end{tabular}

\caption{Example Schema Syntax}
\label{tab:example_schema_syntax}
\end{table}

% nested syntax
The final entry in Table \ref{tab:example_schema_syntax}, {\tt +Sgps+Hlongitude} bears an interesting case. 
This currently specifies that a schema must contain an item matching the type of {\tt gps} and must also contain the field {\tt longitude} {\sl anywhere} in the schema. 
An alternative use might be considered where the user wishes to specify that the schema must contain a item matching the type of {\tt gps}, and must also contain the field {\tt longitude} {\sl within that structure}. 
This would allow a query to be specified that would match a item which renames {\tt altitude} as {\tt height}, but not one which renames {\tt longitude} as {\tt long}. 
The latter could still be matched in the current case, if the {\tt longitude} field existed elsewhere in that schema. 

This usage is implemented through a nested syntax, where constraints can have child constraints attached. 
Again, any items which do not exist in the schema are simply ignored. 
Table \ref{tab:nested_schema_syntax} demonstrates some possible nested queries based on the schema in Figure \ref{fig:locationschema}. 
Clearly in the last two cases, the nested constraint is irrelevant, because they offer no stronger match criteria than their parent. 
In this situation, the nested constraint is simply ignored.

\begin{table}[tbh]
\centering

\begin{tabular}{l p{8cm}}
\hline\hline
Query & Match \\
\hline

{\tt +Sgps}							& {\tt gps} item type \\
{\tt +Sgps[+Hlongitude]}			& {\tt gps} item type containing {\tt longitude} field exactly \\
{\tt +Sgps[+Hlongitude+Hlatitude]}	& {\tt gps} item type containing {\tt longitude} and {\tt latitude} fields exactly \\

{\tt +Sgps[+Hcountry]}				& {\tt gps} item type \\

{\tt +Sgps[+Hlongitude]+Hcity}		& ({\tt gps} item type containing {\tt longitude} field exactly) AND {\tt city} field exactly \\

{\tt +Sgps[+Slongitude]}			& {\tt gps} item type \\
{\tt +Hgps[+Slongitude]}			& {\tt gps} item exactly \\

\hline
\end{tabular}

\caption{Nested Schema Syntax}
\label{tab:nested_schema_syntax}
\end{table}

% how the parser build up the nested structure of constraints to match
The existing parser for RDC queries was extended to allow these new constraint types. 
Upon encountering a {\sl has} or {\sl similar} constraint, the field name, type of match and pointer to another list are added to a schema constraint list. 
A stack is used to handle nested constraints. 
If the schema constraint list is not empty, then an opening bracket causes it to be pushed to the stack, otherwise the bracket is ignored. 
Subsequent constraints are then added to the list pointed to by the last item in the list at the top of the stack, so that each constraint holds a list of constraints nested within it. 
A closing bracket pops the list off the top of the stack, which new constraints are then added to. 
Figure \ref{fig:nested_query} shows how the query {\tt +Sgps[+Hlongitude]} would be represented.  

%TODO: psuedocode or diagram?

% nested query representation
\begin{figure}[tbh]
\begin{lstlisting}
<constraint>
	<name>"gps"</name>
	<exact>false</exact>
	<children>
		<constraint>
			<name>"longitude"</name>
			<exact>true</exact>
		</constraint>
	</children>
</constraint>
\end{lstlisting}
\caption{Nested Query Representation}
\label{fig:nested_query}
\end{figure}

% hashes - overview
Each SBUS endpoint currently has an associated {\sl message hash} of the canonical representation of its schema. 
This hash is computed when the endpoint is created, and determines whether endpoints are able to communicate. 
To create the hash, the schema is read by a top-down parser, which generates tokens representing different items in the schema. 
This parser operates recursively on composite items within the schema, and each token is added to a single buffer. 
Once the parser returns, this buffer contains a canonical representation of the schema, from which a hash can be computed. 

% hashes for subschemas.
These hashes can be used to compare entire schemas, and indeed this takes place when two endpoints attempt to connect. 
However, to compare {\sl parts} of a schema, the schema must be represented not by a single hash, but by a collection of hashes. 
This collection of hashes can also be built when parsing the schema. 
When parsing each individual item, where an item is either a single field or a composite item consisting of multiple fields, a new buffer is used. 
Before the parser returns, a hash is computed of the item contained in that buffer. 
Furthermore, an exact hash is computed using both the names and types, and a type hash is computed using only types. 
This is acheived by using two buffers, with no names being appended to one of them. 
These hashes can then be added to the collection, and the contents of the buffers appended to the parent buffers. 
Using this method, a collection of hashes is built containing the hash of each item within the overall schema. 

This collection is in fact built using lists, where each composite item contains both its hashes and a pointer to a list of its childrens' hashes.
This representation retains the overall structure of the schema, which will be used for nested searches. 
Figure \ref{fig:locationhashes} shows an XML representation of the hash collection which is generated for the schema in Figure \ref{fig:locationschema}.

\begin{figure}[tbh]
\begin{lstlisting}
<location has="9791265A42AB" similar="882A3A2DA208">
	<gps has="55E21713DEAA" similar="931DEF188280">
		<longitude
			has="BE25D0A889CA" 
			similar="00000019316C" />
		<latitude
			has="87AC6D018868" 
			similar="00000019316C" />
		<altitude
			has="B3ACB8255B56" 
			similar="0000001A7774" />
	</gps>
	<city
		has="D3C74C7A7AB4"
		similar="0000001D3C74" />
</location>
\end{lstlisting}
\caption{Hash Structure Generated For Location Schema}
\label{fig:locationhashes}
\end{figure}

% how to query RDC.
To construct a schema query for the RDC, each item in the constraint list is looked up in the hash collection and the appropriate hash is added to the item, depending on whether the item states an exact or type match. 
Figure \ref{fig:nested_query_hash} shows the query in Figure \ref{fig:nested_query} with these hashes. 
This list can then be sent to the RDC, which uses it to search for matching components. 

% nested query hashes representation
\begin{figure}[tbh]
\begin{lstlisting}
<constraint>
	<name>"gps"</name>
	<exact>false</exact>
	<hash>"931DEF188280"</hash>
	<children>
		<constraint>
			<name>"longitude"</name>
			<exact>true</exact>
			<hash>"BE25D0A889CA"</hash>
		</constraint>
	</children>
</constraint>
\end{lstlisting}
\caption{Nested Query Representation with Hashes}
\label{fig:nested_query_hash}
\end{figure}

% how RDC searches.
The RDC has the schema of each endpoint on each registered component, therefore has the hash collection of each endpoint. 
This hash collection is only computed once. 
The RDC returns all matching endpoints, not just the first match, so compares the query with each endpoint schema. 

\begin{algorithm}
\algnotext{EndIf}
\begin{algorithmic}[1]
\Function{CompareSchema}{$constraints$, $schema$}
	\ForAll{$constraint \in constraints$}
		\If{$constraint.hash = schema.hash(constraint.exact)$}
			\If{$constraint.exact$} \label{alg:line:constraint_exact}
				\Continue \Comment match, check next constraint
			\ElsIf{$constraint.children = null$}
				\Continue \Comment match, check next constraint
			\Else \label{alg:line:nested_start}
				\ForAll{$nested \in constraint.children$}
					\State $match\gets false$
					\ForAll{$child \in schema.children$}
						\State $match\gets \Call{CompareSchema}{nested, child}$
						\If{$match$}
							\Continue \Comment match, check next nested constraint
						\EndIf
					\EndFor
					\If{$match \neq true$}
						\Break \Comment no match, skip other nested constraints
					\EndIf
				\EndFor
				\If{$match$}
					\Continue \Comment match, check next constraint
				\EndIf
			\EndIf \label{alg:line:nested_end}
		\Else
			\State $match\gets false$
			\ForAll{$child \in schema.children$}
				\State $match\gets \Call{CompareSchema}{constraint, child}$
				\If{$match$}
					\Continue \Comment match, check next constraint
				\EndIf
			\EndFor
			\If{$match \neq true$}
				\State \Return $false$
			\EndIf
		\EndIf
	\EndFor
	\State \Return $true$
\EndFunction 
\end{algorithmic}
\caption{Pseudocode to Compare Schemas}
\label{alg:compare_schema}
\end{algorithm}

Algorithm \ref{alg:compare_schema} shows how the RDC checks whether a schema satisfies a set of constraints. 
Ignoring lines \ref{alg:line:nested_start}--\ref{alg:line:nested_end}, which check child constraints, the algorithm is simply a depth-first search. 
Each constraint is checked against the schema. 
If the constraint hash matches the appropriate schema hash, then the next constraint is checked, or the algorithm returns {\tt true} if there are no more constraints. 
If the hashes do not match, then the algorithm recursively calls itself using each of the schema's children as the new schema to compare against, until a match is found. 
If no match is found, then the algorithm returns {\tt false}. 
Lines \ref{alg:line:nested_start}--\ref{alg:line:nested_end} match nested constraints very similarly to the entire algorithm as described above. 
However, because these constraints specify fields which must be nested, then cannot be compared to the current item being matched. 
Therefore, the comparison must be conducted against each of the schema's children. 
Once a match is found the next constraint is tested, and if all nested constraints are matched, then the parent constraint is considered matched. 
Line \ref{alg:line:constraint_exact} checks whether a constraint is exact before its children are considered, since the children must match by definition, if it is an exact match. 

% optimisations to search
Queries are automatically optimised by components before being sent to the RDC, to increase performance of the algorithm. This allows the RDC to process the query as quickly as possible, preventing it from becoming a bottleneck in the system. 
Any duplicate constraints are first removed, for obvious reasons. 
Queries are then reordered to place either exact constraints first, constraints on composite items first, or a combination of these. 
This orders constraints such that those which are more difficult to match are checked first, causing the algorithm to fail earlier rather than later, thereby wasting less time. 
The result of these optimisations is explored in the Evaluation. 

%TODO: other ways the search could be optimised - hash table to linked lists, search tree. Potential performance increases + how results might vary.


% construct lookup
Once the RDC returns a list of matches, the component connects to one of the targets. 
During the connection setup, some schema negotiation must occur, so that the components can communicate coherently. 
This work occurs almost entirely on the component which initiated the query, so as not to hinder the target component. 
The component first requests the target's schema. 
Upon receiving this schema, the component checks it against the original query, building a lookup table, while also ensuring that it does in fact match, and the RDC has not been replaced by some malicious process. 
This lookup table consists of a section containing names from the target schema, indexed by names from this schema, and a section containing names from this schema indexed by the target schema, and is used to repackage messages according to this component's schema. 

\begin{figure}[tbh]
\begin{lstlisting}
@place
{
	coordinates
	{
		position
		{
			dbl longitude
			dbl latitude
			int height
		}
	}
	txt country
}
\end{lstlisting}
\caption{Complex Location Schema}
\label{fig:complexlocationschema}
\end{figure}
 
These entries are added every time a successful match occurs in Algorithm \ref{alg:compare_schema}. 
Additionally, entries can be inserted for all descendants of a successful match, since the structure of the schemas must be the same. 
An endpoint using the schema in Figure \ref{fig:locationschema} would create the lookup table in Table \ref{tab:lookup_table} for the schema in Figure \ref{fig:complexlocationschema} based on the query {\tt +Sgps[+Hlongitude]}.

\begin{table}[tbh]
\centering

\begin{tabular}{c c}
\hline\hline
Current & Target \\
\hline
{\tt longitude} & {\tt longitude} \\
{\tt gps} & {\tt position} \\
{\tt latitude} & {\tt latitude} \\
{\tt altitude} & {\tt height} \\

\hline
\end{tabular}

\caption{Lookup Table}
\label{tab:lookup_table}
\end{table}

Once this basic lookup table has been created, corresponding fields within the two schemas can be located, which allows a correspondence between outer structures to be inferred, as shown in Algorithm \ref{alg:construct_lookup_outer}. 
The first loop works upwards from a corresponding field in the schema and target schema. 
Given that these fields match, their ancestors must also match. 
This loop will end once one of the schemas has no parent, adding ({\tt location}, {\tt coordinates}) to the lookup table in this case. 
Only one schema will have remaining ancestors, however if the top level of both schemas has been reached, no further action is taken. 
If the target schema has remaining ancestors, these represent layers which surround the message the component will use. 
These are stored so that a message can be extracted from within these layers. 
Here, {\tt place} is the only extra layer and {\tt place/coordinates} is the root of the message the component wants. 
If the component schema has remaining ancestors, these represent layers will are missing from the message. 
These are stored so that the message can be wrapped in them. 

\begin{algorithm}
\algnewcommand\And{\textbf{ and }}
\algnotext{EndWhile}
\begin{algorithmic}
\While{$current \neq null \And target \neq null$}
	\State \Call{InsertLookup}{$current.name$, $target.name$}
	\State $current\gets current.parent$
	\State $target\gets target.parent$
\EndWhile
\While{$target \neq null$}
	\State \Call{AddExtra}{$target$}
	\State $target\gets target.parent$
\EndWhile
\While{$current \neq null$}
	\State \Call{AddMissing}{$current$}
	\State $current\gets current.parent$
\EndWhile
\end{algorithmic}
\caption{Lookup Table Inference for Outer Structures}
\label{alg:construct_lookup_outer}
\end{algorithm} 

% updating subscriptions
After the lookup table has been created, any predefined subscription filter can be applied to the peer. 
These filters are specified in terms of field names, for example the filter {\tt location/gps/latitude > 0.0} specifies the northern hemisphere. 
If the component schema has additional ancestors, then these are removed from the filter, while if the target schema has additional ancestors, the filter is prepended with these plus the root node, making the filter {\tt place/coordinates/gps/latitude > 0.0}.
Other field names are then replaced with their corresponding entry in the lookup table, creating a filter which when applied to the target schema, has the expected effect, namely {\tt place/coordinates/position/latitude > 0.0}.
Any filters on fields which do not exist in the target schema are applied as specified, so developers must ensure that fields which they intend to filter on are included within their schema query. 
This converted filter will be applied in the final stage of the connection setup, and of course can be updated at any later time. 

% repackage messages
Finally, this lookup table will be used to repackage each message. 
Algorithm \ref{alg:repack_message} shows how each message is repackaged. 
Suppose the component receives the message in Figure \ref{fig:locationmessage}. 

\begin{figure}[tbh]
\begin{lstlisting}
<place>
	<coordinates>
		<position>
			<longitude>0.091732</longitude>
			<latitude>52.210891</latitude>
			<height>19</height>
		</position>
	</coordinates>
	<country>"UK"</country>
</place>
\end{lstlisting}
\caption{Example Location Message}
\label{fig:locationmessage}
\end{figure}

The first stage is to extract the desired message from any additional layers, in this case {\tt place/coordinates}. 
This message is then repackaged under the names defined by this component's schema using Algorithm \ref{alg:repack_message_overall}.
Lines \ref{alg:line:lookup_name_start}--\ref{alg:line:lookup_name_end} find the correct name from the lookup table, returning {\tt null} if it does not exist in the table. 
Lines \ref{alg:line:check_children_start}--\ref{alg:line:check_children_end} then either pack the message content under the new name, if it is a single field, or start a composite item otherwise. 
The loop spanning lines \ref{alg:line:repack_children_start}--\ref{alg:line:repack_children_end} then iterates over each item in the message. 
The name of each item can be used to check if any items are missing from the message, and add them between their siblings. 
The message is otherwise repackaged in the same way, and added to the composite item if it should appear. 
Any other siblings missing from the message can be added at the end. 
Finally, if the component schema had additional layers rather than the peer schema, the message would be wrapped in these missing layers. 
This repackages the message in Figure \ref{fig:locationmessage} to Figure \ref{fig:locationmessagerepack}, which can then be used by the component.  

\begin{algorithm}
\begin{algorithmic}[1]
\Function{Repack}{$message$}
	\If{message has additional layers}
		\State \Call{UnwrapMessage}{$message$}
	\EndIf
	
	\State \Call{RepackageMessage}{$message$}
	
	\If{message is missing layers}
		\State \Call{WrapMessage}{$message$}
	\EndIf
\EndFunction 
\end{algorithmic}
\caption{Pseudocode to Repackage Message}
\label{alg:repack_message_overall}
\end{algorithm}

\begin{algorithm}
\algnotext{EndIf}
\begin{algorithmic}[1]
\Function{RepackageMessage}{$message$}
	\If{$message.name \in lookup\_table$} \label{alg:line:lookup_name_start}
		\State $name\gets \Call{Lookup}{message.name}$
	\Else
		\State \Return $null$
	\EndIf \label{alg:line:lookup_name_end}
	
	\If{$message.children = null$} \label{alg:line:check_children_start}
		\State \Return \Call{Pack}{$message, name$}
	\Else
		\State $composite\gets \Call{CreateStructure}{name}$
	\EndIf \label{alg:line:check_children_end}
		
	\ForAll{$child \in message.children$} \label{alg:line:repack_children_start}
		\State \Call{AddMissingChildren}{$composite$}
		
		\State $node \gets \Call{RepackageMessage}{child}$
		\If{$node \neq null$}
			\State \Call{AddNode}{$composite, node$}
		\EndIf
	\EndFor \label{alg:line:repack_children_end}
	
	\State \Call{AddMissingChildren}{$composite$}

	\State \Return $composite$
	
\EndFunction
\end{algorithmic}
\caption{Pseudocode to Repackage Message}
\label{alg:repack_message}
\end{algorithm}

\begin{figure}[tbh]
\begin{lstlisting}
<location>
	<gps>
		<longitude>0.091732</longitude>
		<latitude>52.210891</latitude>
		<altitude>19</altitude>
	</gps>
	<city>""</city>
</location>
\end{lstlisting}
\caption{Repackaged Location Message}
\label{fig:locationmessagerepack}
\end{figure}

\section{Android Port}

The Android NDK was used to compile SBUS to support Android, and embed the native machine code into applications. 
The documentation for the NDK offers little more than trivial examples, and few other resources exist, so this stage of the project was largely achieved through experimentation. 

A standalone toolchain from the NDK can be used to set the correct compilation variables when running {\tt ./configure}. 
In setting up the toolchain, the architecture of the device and the Android native API platform must be specified. 
ARM-based devices, and Android platform 9 where chosen here, since these corresponded to the available device. 
The standalone toolchain is still in beta which meant there were several problems compiling SBUS directly, typically due to missing libraries. 
Some SBUS code had to be rewritten to account for this, in many cases using {\tt \#ifdef} statements to substitute blocks of code specific to Android. 
The toolchain setup did include a bug\footnote{\url{https://code.google.com/p/android/issues/detail?id=35279}} which meant some include paths were missing, in particular {\tt <limits>} could not be referenced. 
The workaround for this was to fix the directory structure.

The Android version of {\tt pwd.h} which defines {\tt struct passwd} contains no member {\tt pw\_gecos}. 
This contains the user's full name, which is used by SBUS to set the {\tt creator} field in a component's metadata, allowing components to query an RDC by creator. 
To remedy this, the creator was simply replaced by the string `Android'. 
In principle, the user's full name or username could be found in Java using the AccountManager class\footnote{\url{http://developer.android.com/reference/android/accounts/AccountManager.html}}, which could then be passed as a parameter to the component via JNI. 
This would allow the creator to be set more accurately, however it was deemed unimportant to the project. 

% IP address.
One of the greatest problems was that {\tt <sys/sockio.h>} was missing from Android.
SBUS uses this as part of determining the IP address a component is running on, which is then used during connection setup. 
This meant that the entire method to determine the IP address had to be rewritten to work on Android. 
This was achieved by passing an {\tt ifconf} structure to the {\tt ioctl()} system call.
This populates the structure with a list of network interfaces and their IP addresses, which can then be searched for the appropriate interface to find the IP address. 
In this case, the {\tt eth0} interface for Wi-Fi was prioritised over the 3G interface.

% Android FS.
Another large issue was the Android filesystem. 
When creating a component, SBUS invokes a wrapper which is responsible for handling the low level messages between components, passing them to the library level as structured messages.
This typically resides at {\tt /usr/bin/sbuswrapper} which means it can be invoked via a call to {\tt sbuswrapper}, since the {\tt PATH} environment variable contains {\tt /usr/bin}. 
For the same functionality, the wrapper needed to be stored in a directory which is in the {\tt PATH} variable, which on Android, amounts to {\tt /system/bin}. 

The Android SDK provides the Android Debug Bridge ({\tt adb}) which can be used to push and pull files to and from a device. 
When using the Android emulator, {\tt /system} can be remounted as writable\footnote{using {\tt mount -o rw,remount -t yaffs2 /dev/block/mtd3 /system }}, allowing {\tt sbuswrapper} to be pushed to {\tt /system/bin/sbuswrapper}. 
However, on a device without root access, {\tt /system/bin} cannot be remounted in this way, thus cannot be written to. 
Therefore, some other writable directory was needed, and {\tt sbuswrapper} would have to be invoked using the full path. 

The SD card ({\tt /mnt/sdcard}) is writable so was considered, however is mounted with the option {\tt noexec}, which means that although a file can be pushed to it, it cannot be executed. 
The best approach to use was to include {\tt sbuswrapper} as an asset to an application, and copy it to the application's filespace.

Android uses a sandbox concept for security, where each application has access only to its own resources on the device. 
This is enforced by assigning each application a unique user ID, and setting the appropriate permissions. 
Since {\tt sbuswrapper} is approximately \SI{1}{MB}, compared to approximately \SI{30}{MB} available internal memory on the device, it was infeasible to include a copy with every application. 
Therefore, one copy was included with the Phone Manager, which then modified the permissions to make {\tt sbuswrapper} world executable. 
An additional directory was created and made world writable, which is where SBUS stores log files. 

Although the device simply could have been rooted, allowing a much cleaner solution of storing {\tt sbuswrapper} at {\tt /system/bin/sbuswrapper}, it would have devalued the project, since it would only then work on rooted devices.

%TODO: write about closing sockets and all those FIN_WAIT2? Nothing actually changed now, but it was a pain.

% overview of Android architecture
\begin{figure}[tbh]
%set the image x size to the width of the page
\epsfxsize=8cm
\centerline{\epsfbox{figs/android_stack.eps}}
\caption[Android System Architecture]{A brief overview of the Android system architecture, based on the image at \url{http://developer.android.com/images/system-architecture.jpg}}
\label{fig:android_stack}
\end{figure}

% JNI.
The Android NDK was finally used in conjunction with Java Native Interface (JNI) to allow applications to call SBUS functions via Java. 
This involved writing Java classes for components, endpoints and messages, as well as some helper classes. 
The SBUS library is linked via the {\tt Android.mk} file, which {\tt ndk-build} then uses to copy the shared library into the application's root project directory. 
The application can then be built to generate a package file which can run on an Android device. 
% how Android works
Figure \ref{fig:android_stack} gives an overview of how an application works. 
Applications are built using the Application Framework
Every Android application runs in its own process, with its own instance of the Dalvik virtual machine\footnote{\url{http://developer.android.com/about/versions/index.html\#os_architecture}}, which executes the Java bytecode. 
When an SBUS call is made, Dalvik invokes a call to the native JNI code, passing in a {\tt JNIEnv} pointer containing an interface to the Dalvik VM, as well as any other parameters. 
The JNI methods can then invoke their respective SBUS methods. 


Each Java object has a one-to-one correspondence with a native object. 
Native methods which create an object return a pointer to that object, cast to an integer. 
The pointer is passed to the constructor of the Java object representing the native object.
When a method is invoked on the Java object, the pointer is passed back to the native method, which then casts it to a pointer to the native object.
The Java classes hide this complexity from the user, simply providing methods with parameters and return values in terms of these other Java classes.

These classes are exported as a JAR file, which applications can then simply import to use the Java SBUS library.

\section{Phone Manager}

The Phone Manager is an Android application which is responsible for detecting changes in context and reconfiguring other applications' connections. 

% detecting context
The Phone Manager uses the AIRS application\footnote{\url{https://play.google.com/store/apps/details?id=com.airs}} and Android system intents\footnote{\url{http://developer.android.com/reference/android/content/Intent.html}} to detect changes in context. 
These intents are broadcast across the system upon specific events. 
By implementing a receiver, these intents can be captured and an action can be taken upon the event. 
In particular, an intent with the action {\tt NETWORK\_STATE\_CHANGED\_ACTION} is broadcast upon a Wi-Fi change.
This allows the Phone Manager to register a receiver for these intents, which can then reconfigure connections after a change in network. 

% AIRS
An application has been created to act as a AIRS-SBUS gateway between AIRS and the Phone Manager. 
This gateway acts as a remote server for sensor readings from AIRS. 
Once AIRS start in remote mode, sensors must be subscribed to before AIRS will output any readings. 
AIRS firsts connects to the gateway, so that it has somewhere to send messages. 
The Phone Manager can then connect to the gateway via an SBUS endpoint, and subscribe to sensors through another ``control endpoint'' on the gateway. 
Upon receiving a subscription, the gateway relays this subscription to AIRS in the appropriate format. 
AIRS can then send sensor readings to the gateway, which will convert them to SBUS events and send them to the Phone Manager. 
This allows the Phone Manager to use any sensor on the device which is exposed by AIRS, yet ensures that the Phone Manager only receives readings from the sensors it is interested in. 
 
 % AIRS-SBUS gateway
This gateway is based on the AIRS code\footnote{\url{https://github.com/dirktrossen/AIRS}}. 
AIRS exposes upwards of 50 sensors, which is being continually expanded, making it infeasible to predefine an endpoint for each sensor. 
This is handled by dynamically creating an endpoint for a sensor whenever the first reading from that sensor is received, meaning that endpoints will only ever be created as needed. 
The user is presented with a list of sensors in the gateway, which is populated from a message AIRS sends to announce the available sensors, upon connecting to the remote server. 
After the user selects a sensor, that sensor is subscribed to, and the endpoint will be created once a reading is received from that sensor. 
Additionally, the endpoint is automatically mapped to the Phone Manager and can start publishing events to it immediately. 
Each sensor runs in a different thread, so these endpoints are maintained in a thread-safe repository. 
This repository is searched upon receiving a message, which will return the endpoint if it already exists, ensuring that multiple endpoints are not defined for the same sensor. 

% composite event detector
Conceptually this AIRS-SBUS gateway could act as a composite event detector. 
By collating multiple sensor readings, a context change could be defined by multiple different readings satisying some conditions, or by multiple occurrences of an event.
Upon detecting a context change, the gateway would then publish a single event to the Phone Manager. 
The Phone Manager can respond to any kind of event, so already provides all of the infrastructure required to support this.

% registering components + setting policies
In order to have their connections automatically reconfigured, applications must register with the Phone Manager. 
This registration occurs in exactly the same way as with an RDC; the Phone Manager is essentially a lightweight RDC which does not offer search functionality. 
Once an application has registered, it can set {\sl map policies}, which determine how its connection will be reconfigured. 
A policy contains the application endpoint name, a peer endpoint name, and a peer address in the form of an IP address or an RDC query which will be resolved to an address. 
The Phone Manager will apply these policies exactly as specified, either instructing an application endpoint to map to another endpoint at some IP address, or to first resolve an RDC query, then map to the endpoint. 

% RDC updates 
Some context changes may mean that the available RDCs change, for example joining or leaving a network. 
The Phone Manager informs applications of RDC changes on every context change, publishing an event containing the RDC address, and whether it has became available or unavailable. 
Currently an RDC is only assumed to become unavailable if the context change is leaving a Wi-Fi network. 
This event is published to each registered application via another builtin endpoint. 
Upon receiving the event, applications can either automatically accept the change, registering with or deregistering from RDC address, or perform a callback method, perhaps prompting the user to make a decision. 

% where to get RDC address from
This RDC address is currently set by the user in the Phone Manager, however many other ways exist which could be used to find an RDC in the environment. 
Prompting the user may be sufficient on a home network, and additionally this allows the user to choose not to trust an IP address. 
Alternatively, the address could be coded in a QR code or RFID tag. 
The user could scan one of these, for example when making a purchase in a coffee shop, and the Phone Manager would automatically send an RDC update based on the encoded address. 
This approach retains some level of privacy and security, because the user explicitly chooses to scan the QR code or RFID tag. 
An even more automated approach might be to include the RDC address as part of DHCP configuration when connecting to a Wi-Fi network, or to broadcast the address on the network. 
All of these methods must of course be considered from security point of view. 

% apply map policies
After the Phone Manager sends an RDC update to registered applications, it can apply applications' map policies. 
To do this, it simply issues a map events to application endpoints, containing the peer address and peer endpoint name specified in their policies. 
If the peer address specifies an IP address, then the endpoint can map directly to the peer endpoint. 
However, if the peer address is an RDC query, then the application must first resolve the query, which may be done against a new RDC which has been specified in the update. 
This allows the query to be resolved in the new environment, allowing endpoints to map to peers in that environment. 

\cleardoublepage


% ~ Eval + Conclusion ~ 2000 words
\chapter{Evaluation}

The project set out to create a system which would allow applications to automatically reconnect to different sources upon a change in context, where these sources may not necessarily present data in the format expected. 
The project has been successful in this respect. Sensor readings from the AIRS application\footnote{\url{https://play.google.com/store/apps/details?id=com.airs}} and Android intents are used to detect a change in context. 
Once a change is detected, applications will be connected to sources which satisfy the constraints they have specified and messages will automatically be converted to applications' expected format, allowing operation of the application to continue without interruption.

\section{System Overview}

% overview of system image
\begin{figure}[tbh]
%set the image x size to the width of the page
\epsfxsize=\hsize
\centerline{\epsfbox{figs/overview.eps}}
\caption[System Overview]{The light blue boxes represent components and the green boxes within them represent endpoints. The purple boxes are lists of components registered with the RDC and Phone Manager, and the orange box is a list of policies set on the Phone Manager}
\label{fig:system_overview}
\end{figure}

Figure \ref{fig:system_overview} shows the interactions that occur within the system, detailing how a transport application which uses the coach schema in Figure \ref{fig:coach-schema} might have its connection reconfigured upon a change in context.

\begin{enumerate}

\item After the transport application has registered with the Phone Manager, it can send policies to the Phone Manager which specify how its endpoints should be reconfigured upon a change in context. For instance, the application may wish to be connected to any components which offer a structure matching the type of {\tt coach}. In another case, it may simply give an IP address it should be connected to.

\item The AIRS-SBUS gateway acts as a server for sensor readings from AIRS, which then emits the readings as SBUS events, which are consumed by the Phone Manager. The Phone Manager also receives other intents from Android, signalling a change in Wi-Fi network.

\item The Phone Manager processes these events to determine whether a change in context has occurred. Once a change happens, the Phone Manager will inform the transport application about the change by sending it details of whether an RDC should be added or removed. 

\item The transport application receives this message about the RDC change and will either automatically accept this change, registering or deregistering with the RDC, or perform some callback method, perhaps prompting the user to make a decision. The transport application may have chosen to automatically accept the change, but display a message to the user to indicate that the context has changed.

\item The Phone Manager applies all of the policies within its policy table, sending messages to application endpoints, instructing them to connect to different components. The transport application will be sent whatever it has set in its policy.

\item If the transport application has specified an IP address in its policy, the endpoint will receive this IP address and can connect directly to it to start receiving messages. However, if the policy specified that a component must have a structure matching the type of {\tt coach}, the endpoint will query the RDC with this constraint to find matching components.

\item The RDC searches components registered with it for those matching the constraints. In this case, it might find only a component sending coach data, while in other cases it may find components offering train data. In both cases, these components contain a structure matching the type of {\tt coach}, so the RDC returns the IP addresses to the application.

\item The application endpoint then connects to one of the matching components, allowing the user to continue receiving fresh travel data without any interaction needed.

\end{enumerate}

%TODO: Other approaches.
Other messaging systems:
\begin{itemize}

\item RabbitMQ based on AMQP (uses a central broker)
\item ActiveMQ
\item Java Messaging System
\end{itemize}

AWARE\footnote{\url{http://www.awareframework.com/home/}} is a new ``Android framework dedicated to instrument, infer, log and share mobile context information for application developers'' which could be used in place of AIRS to detect a change in context.
AWARE uses broadcasts to notify applications of the current context, and these broadcasts can be captured by Android BroadcastReceivers\footnote{\url{http://developer.android.com/reference/android/content/BroadcastReceiver.html}}. This allows the Phone Manager, or indeed any application, to implement broadcast receivers to be notified of changes in context, rather than using AIRS and the AIRS-SBUS gateway. 
Some applications might choose to implement broadcast receivers themselves, in order to use this framework in place of the Phone Manager to detect context changes.
However, the Phone Manager would still be useful because it would provide this service for any application to use, mapping them as per their policies and reducing complexity in the applications themselves.

% table of example policies
\begin{table}[tbh]
\centering

\begin{tabular}{c c c c}
\hline\hline
Component & Endpoint & Remote Address & Remote Endpoint \\
\hline

Healthcare & HeartRate & 128.232.0.20:44444 & HeartRate \\
Transport & Departures & +NStagecoach & Coach \\
Transport & Departures & +Scoach & NULL \\

\hline
\end{tabular}

\caption{Phone Manager Example Policies}
\label{tab:example_policies}
\end{table}

% about example policies
Table \ref{tab:example_policies} shows example policies which components might set with the Phone Manager. 
Both the local component and local endpoint are implicitly set as whichever endpoint sent the policy. 
The remote address may be an IP address or a query to be resolved by an RDC. 
An IP address may be useful when the application should always be mapped to the same IP address. 
In the first example, a healthcare system has registered to have heart rate readings mapped directly to an endpoint at a known IP address, perhaps a relative or emergency response team. 
However the real flexibility of the system lies in the queries. 
These will be resolved by some RDC, ensuring components are mapped only to other components satisfying their queries. 
In the second row, the transport application sets a policy for any component named ``Stagecoach'' which has an endpoint named ``Coach'', while in the third row it sets a policy for any endpoint which has a structure matching the type of {\tt coach}. 

% table of example policies with events and conditions
\begin{table}[tbh]
\centering

\begin{tabular}{c c c c}
\hline\hline
Event & Condition & Component & Remote Address \\
\hline

Accelerometer & \begin{math} val \ge50 \end{math} & Healthcare & 128.232.0.20:44444 \\
Time & \begin{math} 12 \le val.hour \le 14 \end{math} & Adverts & +IRestaurant \\
Wi-Fi & \begin{math} val = true \end{math} & Transport & +NStagecoach \\
GPS & \begin{math} location(val) = France \end{math} & Transport & +Scoach \\

\hline
\end{tabular}

\caption{Phone Manager Example Policies using Event-Condition-Action Model}
\label{tab:event_condition_action}
\end{table}

% event-condition-action through specifying AIRS sensor + condition
The system is in line with an event-condition-action model, where events are the sensor readings and actions are mapping applications according to their policies. 
The system could easily be extended to allow applications to specify which sensors to use as events, and conditions those events should fulfill for the Phone Manager to apply a mapping policy. 
As shown in Table \ref{tab:event_condition_action}, applications would not only specify the action they wish to take through their map constraints, but also the events and conditions in which they wish to take that action.
The healthcare system could specify that it only wishes for heart rate data to be connected to the IP address if a high accelerometer value was measured, because the reading may indicate that a fall has occurred. 
The transport application can set different policies for different events. If a Wi-Fi connection is established, the application wants to connect specifically to Stagecoach components, however if a GPS event indicates that the user were in France, the application is more lenient and willing to accept any component offering a structure matching the type of {\tt coach}. 
The application may display adverts, where policies could be used to determine the source of the adverts, perhaps from nearby restaurants around lunchtime. 

% user policies
This could further be used to allow the user to set their own policies for applications. 
The application developer cannot possibly account for every scenario, however by allowing the user to set their own policies, through which they are specifying context changes they are interested in, the system becomes fully customisable and personal. 
The user of the transport application might not want to wait for a bus when it is raining. 
Therefore, they could create a policy using a weather event with the condition ``rain''. 
The action might be to connect the application to the IP address of a local taxi company, which provides information about when the next taxi will be available using the schema in Figure \ref{fig:taxi-schema}.

%TODO: UI for user to set policy (system can also set policy).

%TODO: Screenshots of original message + repackaged?

%TODO: How long to go through policy list (all the if statements)

%TODO: How long to tell a component about context change - long list, might take a while for last one - prioritise?

\section{System Performance}

This section examines the performance of the system, specifically aspects related to the schema conversion part of the system. 
The schema conversion plays an important role in mobility because it allows applications to function despite some level of uncertainty, which in turn makes them more adaptable to different environments. 
This schema conversion can only be useful if it can be done in a reasonable time though, since the whole idea behind the system is to provide users with meaningful, {\sl real time} data. 
These tests show the kind of performance which can be expected from the system. 

% rdc search optimisation
\begin{figure}[tbh]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/rdc_search_optimisations.eps}}
\caption{RDC Search Optimisations}
\label{fig:rdc_search_optimisations}
\end{figure}

Figure \ref{fig:rdc_search_optimisations} shows the time taken for an RDC to search through all registered components to find those matching schema constraints, for different numbers of registered components and with different optimisations applied. 
In a real environment, RDCs are likely to have many components registered, so one search cannot impact the performance of any other component too greatly.
As expected, the time taken to search increases linearly with the number of components registered. 
The structures first optimisation narrowly beats the exact first and exact structures optimisations, all of which peform more efficiently than searching without optimisation. 
These optimisations play a greater role as the number of components increases, becoming more important in order to prevent a bottleneck in the system. 
However, even in the worst case the RDC can perform the search with 100 other components registered in just over 40 milliseconds, so should not pose a problem given expected usage.

% construct lookup
\begin{figure}[tbh]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/construct_lookup.eps}}
\caption[Construct Lookup Times]{Time taken to construct a lookup table between schemas of different sized producers and schemas of different sized consumers}
\label{fig:construct_lookup}
\end{figure}

Figure \ref{fig:construct_lookup} shows the time taken to construct a lookup table between schemas of different sized producers and schemas of different sized consumers. 
The lookup table is created before any messages can be exchanged between different schemas, therefore could cause messages to become stale if the process takes too long.
The general increase in time from left to right shows that more time is taken to construct a lookup table as the size of the producer schema increases, as expected, since there are more fields. 
The greater values in the upper right half of the graph show a disparity with the smaller values in the lower left half of the graph. 
This shows that the time taken to construct a lookup table is greater when the producer's schema is larger than the consumer's schema, corresponding to searching in the larger producer schema for the smaller consumer schema to be matched, as opposed to identifying the missing outer structures if the consumer schema were larger.
In either case, the lookup table is constructed once per connection, and takes microseconds.

% repack message
\begin{figure}[tbh]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/repack_message.eps}}
\caption[Repackage Message]{Time taken to repackage a message between schemas of different sized producers and schemas of different sized consumers}
\label{fig:repack_message}
\end{figure}

Figure \ref{fig:repack_message} shows the time taken to repackage a message between schemas of different sized producers and schemas of different sized consumers. 
This repackaging happens once per message, thus causes a delay on every single message received. 
The general increase in time from top to bottom shows that more time is taken when the consumer schema is larger, as expected because more fields are being repackaged under the appropriate names. 
As with constructing the lookup table, messages are repackaged within microseconds, meaning that any message should still be relevant after the time taken for repackaging, for any realistic use of the system. 

Figures \ref{fig:construct_lookup} and \ref{fig:repack_message} both use a schema constraint which matches the type of the smallest schema.

\section{Android Port}

This section evaluates the performance of using SBUS on a phone in comparison to using SBUS on a laptop, investigating whether any significant gains or losses occur as a result of the port.

% table showing map times
\begin{table}[tbh]
\centering

\begin{tabular}{c c c c}
\hline\hline

Consumer & Producer & Mean (ms) & Standard Deviation (ms) \\
\hline

Laptop	&	Laptop	& 7.979 	& 9.787 \\
Laptop	&	Phone	& 20.824 	& 20.064 \\
Phone	&	Laptop	& 21.834	& 15.705 \\
Phone	& 	Phone	& 10.933	& 2.810 \\
\hline
\end{tabular}

\caption{Connection Times Between Components on Different Devices}
\label{tab:map_times}
\end{table}

% figure showing map times
\begin{figure}[tbh]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/map.eps}}
\caption{Connection Times Between Components on Different Devices}
\label{fig:map_times}
\end{figure}

Table \ref{tab:map_times} and Figure \ref{fig:map_times} show mean times and standard deviations for consumers to connect to producers, across phones and laptops.
Each consumer first queries an RDC on a remote machine by the producer name, to find the appropriate producer to connect to. 
Once the consumer receives the IP of the producer from the RDC, it then establishes the connection with it, hence two connections occur within each mapping. 
No schema conversion occurs as part of this map. 
These figures show that a greater time is required when the producer and consumer are on different devices than when they are on the same device. 
This bears no surprise since the local connections are faster than the network connections. 
The mean time taken is slightly greater when both the producer and consumer are on the phone rather than both on the laptop, though the standard deviation is less. 
Overall, an application written in Java running a phone would see very similar network delays to that of a program written in C++ running on any other machine.

% figure showing jni times
\begin{figure}[tbh]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/jni.eps}}
\caption{Function Call via JNI Timing Breakdown}
\label{fig:jni_times}
\end{figure}

Figure \ref{fig:jni_times} shows the breakdown of time taken to perform an SBUS function call via Java compared to C++. 
In this case, the function call is to emit a message. 
This clearly shows that using JNI to call a native method from Java introduces a delay, almost five milliseconds here. 
Each call to an SBUS function via Java will incur some delay, meaning that applications written in Java for a phone will never operate as quickly as those written in C++.
Application developers can expect this delay, which is likely to show less variance than any network delay, and can build their applications to take account of it. 
Provided an application is calling SBUS functions less frequently than this delay, such as a emitting a message less than once every ten milliseconds, it is unlikely to cause any problems.

%TODO: JNI times for other method calls?

%TODO: Stress test? How many components on Android can communicate?

%TODO: Example code for Android SBUS component?

%TODO: Screenshots?

\cleardoublepage

\chapter{Conclusion}

% summarise
The project has created a context-aware system which enables developers to write applications without prior knowledge of the location of data sources or the format of their data, decoupling applications from their data sources. 
The system supports the goals of pervasive computing, achieving scalability by allowing applications to use any relevant data source, and allowing applications to adapt to use heterogeneous content, all while being invisible to both the user and developer.

A policy engine monitors the environment for events which the system or user believe to represent a change in context.
Upon detection of an event, the policy engine applies applications' policies, reconfiguring their connections as appropriate. 
Through these policies, applications may control which data sources they should be connected to, without requiring applications to find data sources themselves.
Policies may be simple, such as the name of a data source, or more complex, specifying how fields in a data source's schema must match their own. 

The system repackages messages so that applications will only ever be presented with data conforming to their own schema, removing uncertainty about the format of the data.
Furthermore, applications may specify subscription filters in terms of their own schemas, placing restrictions on which messages they will be sent, without requiring knowledge of peers' schemas.
When reconfiguring a connection, the system translates these filters to match the peer's schema and applies them to the peer, filtering messages as dictated by the application.

% example
Referring back to the transport example from the Introduction, there may be a transport application based on the coach schema (Figure \ref{fig:coach-schema}) which requires data sources to offer a structure matching the type of the {\tt coach} structure. 
At a coach station, all data sources match this schema exactly, so the application can connect to any one of them. 
The user may then travel to a larger station, offering train and coach services. 
Although train data sources use a different schema (Figure \ref{fig:train-schema}) to the application, their schema contains a structure matching the required {\tt coach} structure.
Upon arrival at the station, the user connects to the Wi-Fi network. The policy engine detects this change in context and connects the application to a train data source. 
Messages will be repackaged to match the application schema, meaning that the same code handles messages for coaches or trains.
The user might have subscribed to only receive data for which the departure time is after midday. 
Since this subscription is specified in terms of the coach schema, it cannot be applied directly to the train schema. 
In this case, the system reformats the subscription to match the train schema and applies it when connecting the application to the data source, ensuring the user's subscription filter is met.

The application might equally have used the train schema, yet only required the {\tt info} structure, allowing it to use coach data sources.

\section{Issues}
% remaining problems
The system currently faces issues when context is rapidly changing, for example, moving in and out of range of a Wi-Fi network. 
The network connection drops causing the system to reconfigure connections, which may disconnect applications from their peers.
However, the network connection is almost immediately re-established causing the system to reconfigure connections again, possibly connecting applications to these same peers. 
If this reconnection occurs before disconnection has completed, the application believes itself to still be connected to the peer, thus will not create a new connection.

A solution to this problem might be to implement conditions which must be satisfied before reconfiguration occurs. 
In a Wi-Fi network, there may be a condition that the network signal must be greater than a given threshold, meaning no reconfiguration would occur until the user moves firmly into the network. 
Alternatively, conditions on multiple context changes might be required before performing reconfiguration.  
For an accelerometer context change, a high accelerometer message followed by a message indicating little movement might be used to detect a fall.

\section{Changes}
%TODO: what I'd have done differently
{\sl I think this section might be more obvious after writing in the Preparation chapter about how the project was planned. }


\section{Future Work}
% other stuff that could be done

% alternate events for change in context - AIRS
Applications cannot currently specify {\sl which} changes in context they are interested in, the policy engine triggers {\sl all} policies upon detecting a change.
The system could easily be extended to allow applications to include a type of context change in a policy, allowing them to specify different policies for different context changes. 
Upon receiving an event, the policy engine would then check whether the event satisfies the conditions of each policy, only applying the policy when it does. 
This would allow applications to specify policies for simple events, such as a Wi-Fi change event, or more complex events, such as a high accelerometer reading followed by no movement to represent a fall. 

As well as applications specifying policies, users could also specify policies for different applications. 
This moves the system further towards the pervasive vision. 
Applications would take sensible actions for different changes in context, however the user could refine how applications should react to changes in context, causing them to behave in a way more suited to the user's needs.

The user may have the ability to specify which types of events can be used by the system. 
This offers some privacy to the user by allowing them to block applications from knowing if some part of their context has changed, for example blocking the accelerometer.

% qualify names if multiple fields in sensor have same name
Applications currently specify RDC schema queries in terms of field names of their own schema.
While this works provided there are no repeated fields names within the schema, it would fail on a schema such as Figure \ref{fig:repeatednameschema} due to {\tt sensor} being repeated. 
A solution to this would be to force field names to be fully qualified in a query. 
A query using {\tt sensor} would mean the entire structure, while {\tt sensor/sensor} would mean the text field.

\begin{figure}[tbh]
\begin{lstlisting}
@sensor
{
	txt sensor
	int value
}
\end{lstlisting}
\caption[Schema with repeated name]{A schema with a structure named {\tt sensor} and a text field also named {\tt sensor}. A constraint specified on {\tt sensor} would be ambiguous without fully qualified names}
\label{fig:repeatednameschema}
\end{figure}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
% nocite means add all references even if not cited.
\nocite{*}
\bibliography{refs}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\begin{appendix}

\chapter{Possible Titles}

\begin{itemize}
\item Automatic Communication Configuration in Mobile Environments
\item Data Stream Connection and Message Negotiation
\item Flexibile Data Streams in Mobile Environments
\item Automatic Data Stream Configuration in Mobile Environments
\end{itemize}

\cleardoublepage

\chapter{Example Schemas}

\input{schemas}

\cleardoublepage

% should this be an appendix?
\chapter{Project Proposal}

\input{propbody}

\end{appendix}

\end{document}
