\documentclass[12pt,twoside,notitlepage]{report}

\usepackage{a4}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage[chapter]{algorithm}
\usepackage{algpseudocode}

\newcommand{\disstitle}{Automatic Contextual Reconfiguration in Mobile Applications}
% POTENTIAL TITLES:
% Automatic Configuration in Mobile Environments
% Automatic Communication Configuration in Mobile Environments
% Data Stream Connection and Message Negotiation
% Flexibile Data Streams in Mobile Environments
% Automatic Data Stream Configuration in Mobile Environments

\newcommand{\wordcount}{11038}

% create extra algorithm commands
\algnewcommand\algorithmiccontinue{\textbf{continue}}
\algnewcommand\Continue{\State\algorithmiccontinue}

\algnewcommand\algorithmicbreak{\textbf{break}}
\algnewcommand\Break{\State\algorithmicbreak}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
    pdftitle=\disstitle,
    pdfauthor={Tom Smith},
    pdfsubject={Computer Science},
}

% set tab size for lstlisting
\lstset{tabsize=4,basicstyle=\ttfamily}

\input{epsf}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.2}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf Thomas Smith}

\vspace*{60mm}
\begin{center}
\Huge
{\bf \disstitle} \\
\vspace*{5mm}
Computer Science Tripos Part II \\
\vspace*{5mm}
Sidney Sussex College \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{l p{9.5cm}}
Name:               & \bf Thomas Smith	\\
College:            & \bf Sidney Sussex College	\\
Project Title:      & \bf \disstitle	\\
Examination:        & \bf Computer Science Tripos Part II, 2013 	\\
Word Count:         & \bf\wordcount\footnotemark[1] \\
Project Originator: & Dr Jatinder Singh		\\
Supervisor:         & Dr Jatinder Singh		\\ 
\end{tabular}
}

\footnotetext[1]{This word count was computed by {\tt detex -e `appendix,figure,algorithm' diss.tex | tr -cd `0-9A-Za-z $\tt\backslash$n' | wc -w}}
\stepcounter{footnote}


\section*{Original Aims of the Project}

The aim of the project was to investigate how applications can respond to changes in context, primarily those arising as a result of mobility. 
This requires some system which allows applications on a mobile device to communicate with those in different environments. 
There must be some mechanism for detecting context changes. 
Applications need to be able to discover and connect to relevant peers. 
Finally, applications should be able to make meaningful use of data which is slightly different to their expectations. 
This grants applications the ability to interact with a wider range of peers, allowing greater adaptability to different environments.

\section*{Work Completed}

A messaging middleware was modified to allow searching for peers by aspects of their schema, offering exact matches based on names and types, or looser matches which only consider types.
This middleware was ported for use on Android, allowing applications on a mobile device to communicate with other systems using the same middleware. 
An application was created to monitor changes in context through various sensors on the device. 
Upon detection of a change in context, this application informs other applications of the change or automatically reconfigures their connections to connect them to new peers, based on policies the appplications define.

\section*{Special Difficulties}

The main difficulty in the project came from porting the messaging middleware to Android. The Android NDK\footnote{\url{http://developer.android.com/tools/sdk/ndk/index.html}} toolkit was used in order to compile the C++ library for use on Android, however missing standard libraries and poor documentation made this task more difficult than expected.
 
%\newpage
\section*{Declaration}

I Thomas Smith of Sidney Sussex College, being a candidate for Part II of the Computer
Science Tripos, hereby declare that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose. 
I give permission for my dissertation to be made available in the archive area of the Laboratory's website.

\bigskip
\bigskip
\leftline{Signed: }

\bigskip
\bigskip
\leftline{Date: }

\cleardoublepage

\tableofcontents

\listoffigures

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}

% Intro + Preparation ~ 1400 words
\chapter{Introduction}

% summarise
While many applications exist for mobile devices, few applications themselves are truly mobile. 
The user may happen to be mobile, however applications do not adapt to different environments. 
Applications may typically use data from predetermined central servers, failing to exploit local data which may be available in the environment. 
This project addresses the issues faced in creating applications which can change to suit different environments, demonstrating how applications can perceive an environmental change, how they can find relevant, local data, and how they can achieve some flexibility in the data which they use. 
This allows applications to be more adaptable to their environment, and therefore offer a more relevant experience to the user. 

\section{Problem}
% what's the problem

% describe current approach
Current applications tend to be inflexible in where the data they use comes from, often selecting from some hardcoded list of IP addresses regardless of the environment. 
While some localisation might be achieved by including parameters during connection, such as sending GPS co-ordinates, the possible parameters which will be accepted have already been fixed, restricting how well applications can adapt to their environment. 
The format of the data which is being exchanged has also been fixed, restricting applications even further.

This is not in line with the pervasive computing vision \cite{weiser1991computer}, primarily because applications do not use their current environment. 
The systems are not scalable, because applications are using global servers, and they offer no heterogeneity\cite{saha2003pervasive} in the data they use, since both application and server both use an exact format.

Applications can gain more relevant data by connecting to local peers, due to the fact that these peers reside within the same environment.
Given an application which requires the current temperature, a global server may use current GPS co-ordinates to determine the nearest weather station and return its most recent reading. 
Yet if the application were being used indoors, a value from a local temperature sensor might not only suffice, but be more appropriate. 
Perhaps this could be achieved otherwise, if the server had known about the temperature sensor, however why should the application need to connect to a global server in order to use local data? Perhaps there isn't even an Internet connection available. 
Furthermore, given that the user is mobile, the server would need to know about {\sl every} temperature sensor {\sl everywhere}. 
Local peers are more related to applications' environments. 
Furthermore, applications need not predefine a list of peers to use.

% context
Context refers to any event which the system or the user feels that it is reasonable to respond to, where any happening of interest that can be observed from within a computer is considered an event \cite[p11]{muhl2006distributed}. 
Mobile devices constantly operate in different contexts, which dictate how the devices can interact with the environment. 
Upon a change in context, an application may move out of scope of a given peer, or the data it is providing may no longer be relevant. 
In this situation, the application must find an alternative peer which can provide similar data, or which provides data which is more relevant to the new context. 

Once this alternate peer has been located, the application must still determine whether it provides useful data. 
Some data may be fundamental to the operation of the application, while other parts may be desired, but not crucial.  
Even once the data is deemed useful, the application may still face the difficulty of data being presented in an unexpected format, simply because of the geographical scale and market which the application might be used in. 
Different manufacturers might present data in slightly different formats, or data may be presented in different languages in different countries. 
These factors mean that when comparing similar pieces of data, there may be missing fields, there may be additional fields, or fields may use different names. 
An application may also wish to specify a subscription filter about the data it should receive, yet without knowledge of the data format, it cannot specify filters on different fields.

% example
For example, a transport application may face differences in the data format for coaches (Figure \ref{fig:coach-schema}) and for trains (Figure \ref{fig:train-schema}). 
While both offer similar data to a large extent, fields are named differently, and the train schema additionally offers a menu for an on-board shop.
Alternatively, the application might be used abroad, for example in a French coach station. The data will still be very similar, but fields names may be in French (Figure \ref{fig:french-coach-schema}).

An application developer may be able to account for some of these differences, though given the sheer volume of potential peers, the developer cannot hope to account for every possibility, meaning that the application cannot be truly adaptable. 
Even if the developer could cover every existing peer, these peers are liable to change over time. 
New peers may become available and old peers may become unavailable, or peers might change how they present their data. 
A developer simply cannot cover every current and future possibility, meaning an application cannot truly adapt to {\sl any} environment. 
However, by specifying how an application should react to changes in context, the application becomes decoupled from the environment, making it more robust to changes in the environment. 


\section{Project}
% what I've done

Schilit et al \cite{schilit1994context}  define a context-aware system as one which can examine the computing environment and react to changes in the environment. 
This project investigates how automatic contextual reconfiguration can be performed for applications whenever there is a change in the environment. 
Pascoe \cite{pascoe1998adding} lists four context-aware capabilites. 
{\sl Contextual sensing} presents environmental state to the user, while {\sl contextual adaptation} allows applications to use this knowledge to adapt their behaviour to integrate with the user's environment.
{\sl Contextual resource discovery} allows applications to discover and exploit resources which exist in the same context as themselves. 
{\sl Contextual augmentation} augments reality with digital data. 
Contextual resource discovery discovers potential peers available within the current context, and contextual adaptation allows applications to adapt to different data formats in order to communicate with these peers. 

% pervasive computing
Use of local peers ensures that the approach is scalable. 
Connections are created on an ad-hoc basis, based entirely upon which peers are available within the environment at that time. 
Adaptation to different data formats masks heterogeneity in the data \cite{saha2003pervasive} from users, and indeed from the application developers. 
These factors are important because uniformity and compatibility in every environment cannot be assumed, and therefore must be considered by any system which aims to be in line with the goals of the pervasive computing vision \cite{weiser1991computer}.

% describe context, say only focusing on mobility (Wi-Fi) for now.
M\"{u}hl represents changes in context using events, where each context change causes an event to be raised \cite[p11]{muhl2006distributed}.
Three kinds of events are defined: physical events, timer events, and a state change in the system, all of which may warrant some kind of reconfiguration. 
In a healthcare system, a rapid accelerometer change followed by no movement may represent a physical event (a fall), in which case the application may need to send data to an emergency response team.
A timer event indicates the progression of real time, which advertisers could use to connect to advertisements for nearby restaurants at lunch time. 
Finally, there are state changes in the system, such as a change in network connection or location. 
This project primarly focuses on state changes events, particularly using a change in network connection to indicate a change in context, though in practice any event might be used.  
Composite events might even be considered to represent a context change. 
These require the detection of several different events, or one or more occurrences of a single event \cite{chakravarthy1994composite}. 

% policy engine
Applications control how their connections should be reconfigured through policies. 
An application monitors the environment and upon detecting a change in context applies these policies, reconfiguring applications' connections to connect them to different peers, where appropriate.
These policies may specify the name of peers, or be more complex rules regarding the structure of peers' data. 
Alternatively, applications may opt to be notified of a change in context, rather than have their connection reconfigured, allowing them to perform callback methods upon the change.

% repackaging
Depending on its policy, an application may be connected to a peer which provides data in a format which differs from its own.
This can be handled by reformatting the data to match the expected format. 
Any extraneous fields are removed, any absent fields are filled with ``empty'' values, and the policy is used to infer relationships between fields with different names. 
This reformatted data can then be passed to the application, allowing the application to continue as if the data received has conformed to its own data format. 
% subscription
Additionally, any subscription filter which an application has placed on data, in terms of its own format, are converted to match the peer's data format, so that the application can be connected with a working subscription in place. 


% summarise
Applications' connections are automatically reconfigured upon a change in context, and data is automatically repackaged to comply with the format which an application expects. 
Local peers can provide data relevant to the environment, and by repackaging this data applications are no longer bound to exact data formats. 
This grants applications flexibility in the peers which they connect to, making them more adaptable to the environment. 
Applications control how they interact with an environment through policies, while still seamlessly communicating within that environment.

\cleardoublepage

 
\chapter{Preparation}

This chapter first outlines the requirements of a context-aware system, going on to summarise related work which the project will build upon, namely the SBUS middleware \cite{ingram2009reconfigurable}, and finally describing how such a system will be implemented to meet these requirements.

%TODO: Why the recursive discovery through peer inspection hasn't been implemented, and how it could be (should be somewhere else?)

\section{Requirements Analysis}

The following use cases outline the functional requirements of the system, describing how the system should operate in different circumstances.

% use cases - functional requirements
John uses a National Heritage application which can provide information about exhibits in a museum, or nearby landmarks in a city. 
While in a museum, John's phone is connected to the Wi-Fi network, and the application receives data from a source on the network, which provides a guided tour of the museum. 

\begin{itemize}

\item {\bf Detect and Act upon Change in Context}

\begin{enumerate}

\item John uses the application to view information about each exhibit while walking around the museum.

\item Upon leaving the museum, his phone disconnects from the Wi-Fi network.

\item The system detects that John has left, so connects the application to a global source via 3G, which uses his GPS co-ordinates to provide the application with information about nearby landmarks, so that John can decide where to go next. 

\end{enumerate}

\item {\bf Connect to Relevant Peers}

\begin{enumerate}

\item John chooses to visit a nearby art gallery, and upon entering the gallery connects his phone to the Wi-Fi network.

\item The system detects this change, and searches for a source providing ``museum data''.

\item Once the system finds a source, it connects the application to it, allowing John to use the application for a guided tour of the art gallery. 

\end{enumerate}

\item {\bf Allow Communication Between Incompatible Schemas}

\begin{enumerate}

\item The art gallery is trialling a new system which includes relevant links in the data for each exhibit.  

\item John's application does not support this extra field, making the source incompatible with the application.

\item The system converts these art gallery messages to match the application data format, making the data compatible with John's application. 

\end{enumerate}

\end{itemize}

%TODO: more of this.
% non-functional requirements
Due to the ever changing nature of mobile environments, the system must also meet certain non-functional requirements to ensure that is is responsive and usable. 
The system must search for peers to connect to within a reasonable time, and any message conversion required for a given peer must incur negligible delay.
These constraints are necessary so that any communication between peers remains relevant to the user's context.

\section{SBUS}

SBUS \cite{ingram2009reconfigurable} is a messaging middleware written in C++, which can be used for stream based communication. 
The basic entity in SBUS is a {\sl component}. 
Each component consists of one or more {\sl endpoints} which can be connected, or {\sl mapped} together allowing direct communication between the two endpoints. 
Every endpoint has an associated schema. 
In order for endpoints to be mapped together, their schemas must match. 
An endpoint may specify a subscription filter on events it wishes to receive, in terms of its schema. 
Any connected peers will only publish events to this endpoint if the event matches the subscription.

These connections and subscriptions are dynamically reconfigurable and may be modified by any other component, allowing third party remapping to occur, subject to security policies. 
A component may issue a message to some other component, instructing it to map or unmap its endpoints, or to update its subscription filters.

Resource discovery components (RDCs) are components which act as a name service. 
An environment may contain several RDCs, or no RDCs, though typically one RDC exists in each environment.
Components may query an RDC for other components within the environment, using parameters including component name, connected peers, or public key. 
The RDC will return IP addresses of matching components, allowing the component to establish a direct connection to a match.

\section{System Design}

The system will be based upon the SBUS middleware, using and extending it to meet the requirements. 
Applications will implement an SBUS component, sending and receiving messages on endpoints. 

% Android NDK
SBUS will be ported for use on Android. 
The Android Native Development Kit\footnote{\url{http://developer.android.com/tools/sdk/ndk/index.html}} (NDK) will be used to compile SBUS for Android, and the Java Native Interface (JNI) framework will be used to allow applications to use this library. 
Since the NDK offers fairly obscure documentation with little more than trivial examples, some time will be required to understand how the build process works. 

% How applications will find peers relevant to them (constraints in terms of schema)
When components query the RDC they generally know something about the metadata of components they want, such as names, connected peers, or public keys. 
In a mobile environment, this is unlikely to be the case. 
Given the amount of possible environments, and the amount of different companies operating in these environments, an application cannot rely on static information about the components. 
The components may, and are perhaps even likely to change over time, meaning that a more robust solution is required. 
To allow applications to find relevant components in any environment, SBUS will be extended to allow RDC queries centred around data. 
The ability to search for components by the format of their data will allow components to discover other components which provide data they require, without any prior knowledge of the environment. 
This is well suited to mobile applications because the environment is constantly changing. 
Furthermore, these environments are not all controlled by a single company and this solution allows resource discovery independent of any corporate differences. 

% How applications will cope with unexpected data format - must be done at system level
SBUS endpoints can currently only communicate if they share a common schema. 
This ensures that endpoints only ever receive messages matching their own schema, from which values can be easily extracted since the structure of the message is known. 
Different components in different environments may offer similar data, but they may have schema differences, perhaps due to the company providing the component, or the country which the component is located in. 
When searching for components by data, an application may discover some component which provides the required data, but nested within a larger schema, or using different field names. 
In the current SBUS implementation, the application would not be able to communicate with this component. 
% TODO: example
Even if it were able to, the application will not know the structure of the message, so cannot locate a particular value within it without performing some complex parsing. 
This is obviously undesirable since this parsing will be required for every single message, and each environment might use a slightly different schema.
To avoid each application implementing such a parser, these differences will be handled by SBUS, before the application receives the message. 
SBUS will be extended to allow components to communicate despite schema differences by reformatting any incoming message to match the application schema.
This means that an application will only receive messages matching its schema, never facing any uncertainity in the structure of a message. 

% How change in context will be detected
A typical smartphone offers many sensors which can be used to determine a context change. 
Android applications can register to receive intents\footnote{\url{http://developer.android.com/reference/android/content/Intent.html}} which indicate events such as a network change, or an incoming phone call. 
Furthermore, the AIRS application\footnote{\url{https://play.google.com/store/apps/details?id=com.airs}} observes sensor readings, and can output them to a remote server.
The system can use these intents, and act as a server for AIRS to obtain these sensor readings, allowing it to monitor the environment, and determine context changes. 
Upon detecting a context change, it can then remap other applications on the phone connecting them to peers suited to the environmental context. 

% How applications' connections will be reconfigured (spoke map)
In order for the system to remap applications, it will first need to know about them. 
An RDC is well suited for this, because it maintains a list of registered components, which can then be searched when a component queries the RDC. 
This list can also serve as a list of components which the system can remap.

These two aspects of detecting context change and acting as an RDC can be combined into a Phone Manager application. 
The Phone Manager will act similarly to an RDC, allowing applications on the device to register with it. 
It will detect context changes and upon a change will issue messages to registered components to instruct them to remap to other components.

% How the Phone Manager knows what to reconfigure to (policies)
The Phone Manager will remap applications upon a change in context, but must remap them to appropriate places. 
The Phone Manager cannot decide what constitutes ``appropriate'' for any given application, as such, the application must decide for itself. 
Applications will register {\sl policies} with the Phone Manager which dictate how they should be remapped upon a context change. 
This policy might be an IP address to map to, or a query which can be performed against an RDC in the environment, which will then return matching components. 
The Phone Manager will simply remap applications as specified by their policies.

% Security
This project will not focus on security. 
A secure version of SBUS which uses OpenSSL to encrypt communications between components exists, and all of the work undertaken in this project could be applied to that version of SBUS.

\cleardoublepage


% ~ 4000 words
\chapter{Implementation}

This chapter first shows how applications can find peers which offer the data they require, and how they can communicate with these peers despite differences in schemas. 
It goes on to explain how the SBUS library was ported to Android, and how it can be used by application developers, before bringing everything together with the Phone Manager, which is responsible for detecting changes in context and reconfiguring applications' connections upon a change.
Finally, this chapter examines some remaining issues in the system and potential solutions, which have not been implemented due to time constraints. 

\section{Flexible Schema Matching}

Flexible schema matching refers to components matching peers based on aspects of their schema, specifically whether or not their schema contains certain fields, and how components can communicate with these peers if their schemas are not identical. 

% syntax
The most natural way for a component to query an RDC with data constraints seemed to be to extend the current query mechanism. 
When searching, a component may be interested in an exact match, where both names and types match, or a weaker match where only types must match. 
Since a component only knows about its own schema in advance, these queries must be specified in terms of its own schema.
Two new query parameters have been introduced to allow this functionality. 
An exact match is specified by {\tt +H}, representing a schema {\sl has} an item, and a type match is specified by {\tt +S}, representing a schema has a {\sl similar} item.
Each of these is used with the name of a single item in the component's schema.
If this item is a single field, it refers only to that field, whereas if it is a composite item consisting of multiple fields, the query applies recursively to the entire item. 
Any item which does not exist in the schema is ignored. 
Table \ref{tab:example_schema_syntax} demonstrates some possible queries based on the schema in Figure \ref{fig:locationschema}.

\begin{figure}
\begin{lstlisting}
@location
{
	gps
	{
		dbl longitude
		dbl latitude
		int altitude
	}
	txt city
}
\end{lstlisting}
\caption{Location Schema}
\label{fig:locationschema}
\end{figure}

\begin{table}
\centering

\begin{tabular}{l l}
\hline\hline
Query & Match \\
\hline

{\tt +Scity}		& {\tt city} field type ({\tt txt}) \\
{\tt +Hlongitude}	& {\tt longitude} field exactly \\

{\tt +Sgps}			& {\tt gps} item type \\
{\tt +Hgps}			& {\tt gps} item exactly \\

{\tt +Hcountry}		& {\tt <ignored>} \\
{\tt +Hlocation}	& {\tt location} schema exactly \\

{\tt +Sgps+Hlongitude}	& {\tt gps} item type AND {\tt longitude} field exactly \\

\hline
\end{tabular}

\caption{Example Schema Syntax}
\label{tab:example_schema_syntax}
\end{table}

% nested syntax
The final entry in Table \ref{tab:example_schema_syntax}, {\tt +Sgps+Hlongitude} bears an interesting case. 
This currently specifies that a schema must contain an item matching the type of {\tt gps} and must also contain the field {\tt longitude} {\sl anywhere} in the schema. 
An alternative use might be considered where the user wishes to specify that the schema must contain a item matching the type of {\tt gps}, and must also contain the field {\tt longitude} {\sl within that item}. 
This would allow a query to be specified that would match a item which renames {\tt altitude} as {\tt height}, but not one which renames {\tt longitude} as {\tt long}. 
The latter could still be matched in the current case, if the {\tt longitude} field existed elsewhere in that schema. 

This usage is implemented through a nested syntax, where constraints can have child constraints attached. 
Again, any items which do not exist in the schema are simply ignored. 
Table \ref{tab:nested_schema_syntax} demonstrates some possible nested queries based on the schema in Figure \ref{fig:locationschema}. 
Clearly in the last two cases, the nested constraint is irrelevant, because they offer no stronger match criteria than their parent. 
In this situation, the nested constraint is simply ignored.

\begin{table}
\centering

\begin{tabular}{l p{8cm}}
\hline\hline
Query & Match \\
\hline

{\tt +Sgps}							& {\tt gps} item type \\
{\tt +Sgps[+Hlongitude]}			& {\tt gps} item type containing {\tt longitude} field exactly \\
{\tt +Sgps[+Hlongitude+Hlatitude]}	& {\tt gps} item type containing {\tt longitude} and {\tt latitude} fields exactly \\

{\tt +Sgps[+Hcountry]}				& {\tt gps} item type \\

{\tt +Sgps[+Hlongitude]+Hcity}		& ({\tt gps} item type containing {\tt longitude} field exactly) AND {\tt city} field exactly \\

{\tt +Sgps[+Slongitude]}			& {\tt gps} item type \\
{\tt +Hgps[+Slongitude]}			& {\tt gps} item exactly \\

\hline
\end{tabular}

\caption{Nested Schema Syntax}
\label{tab:nested_schema_syntax}
\end{table}

\begin{algorithm}
\begin{algorithmic}[1]
\While{$token \neq null$}
	\If{token is a constraint}
		\State $\Call{list.add}{constraint}$
		\State $list.last \gets constraint$
	\Else
		\If{token is open bracket}
			\State $\Call{stack.push}{list}$
			\State $list \gets list.last.children$
		\ElsIf{token is close bracket}
			\State $list \gets \Call{stack.pop}$
		\EndIf
	\EndIf
\EndWhile
\end{algorithmic}
\caption{Parse Nested Constraints Using a Stack}
\label{alg:parse_nested}
\end{algorithm} 

% how the parser build up the nested structure of constraints to match
The existing parser for RDC queries was extended to allow these new constraint types. 
Upon encountering a {\sl has} or {\sl similar} constraint, the field name, type of match and pointer to a list of children are added to a list. 
Algorithm \ref{alg:parse_nested} shows how a stack is used to handle nested constraints. 
An opening bracket causes the list of constraints to be pushed to the stack.
Subsquent constraints are then added to the child constraints of the last item in that list. 
A closing bracket pops the list off the stack, and constraints are appended to this list again. 
Some error checks also exist to ensure that additional opening brackets are ignored if the list is empty, since they have no meaning, and that closing brackets are ignored if there are too many, since there may not be a list on the stack. 
The checks would reduce {\tt [+Sgps[[+Hlongitude]]]]} to {\tt +Sgps[+Hlongitude]}.
Figure \ref{fig:nested_query} shows how the query {\tt +Sgps[+Hlongitude]} would be represented.

% nested query representation
\begin{figure}
\begin{lstlisting}
<constraint>
	<name>"gps"</name>
	<exact>false</exact>
	<children>
		<constraint>
			<name>"longitude"</name>
			<exact>true</exact>
			<children></children>
		</constraint>
	</children>
</constraint>
\end{lstlisting}
\caption{Nested Query Representation}
\label{fig:nested_query}
\end{figure} 

% hashes - overview
Each SBUS endpoint currently has an associated {\sl message hash} of the canonical representation of its schema. 
This hash is computed when the endpoint is created, and determines whether endpoints are able to communicate. 
To create the hash, the schema is read by a top-down parser, which generates tokens representing different items in the schema. 
This parser operates recursively on composite items within the schema, and each token is added to a single buffer. 
Once the parser returns, this buffer contains a canonical representation of the schema, from which a hash can be computed. 

% hashes for subschemas.
These hashes can be used to compare entire schemas, and indeed this takes place when two endpoints attempt to connect. 
However, to compare {\sl parts} of a schema, the schema must be represented not by a single hash, but by a collection of hashes. 
This collection of hashes can also be built when parsing the schema. 
When parsing each individual item, where an item is either a single field or a composite item consisting of multiple fields, a new buffer is used. 
Before the parser returns, a hash is computed of the item contained in that buffer. 
Furthermore, an exact hash is computed using both the names and types, and a type hash is computed using only types. 
This is acheived by using two buffers, with no names being appended to one of them. 
These hashes can then be added to the collection, and the contents of the buffers appended to the parent buffers. 
Using this method, a collection of hashes is built containing the hash of each item within the overall schema. 

\begin{figure}
\begin{lstlisting}
<location has="9791265A42AB" similar="882A3A2DA208">
	<gps has="55E21713DEAA" similar="931DEF188280">
		<longitude
			has="BE25D0A889CA" 
			similar="00000019316C" />
		<latitude
			has="87AC6D018868" 
			similar="00000019316C" />
		<altitude
			has="B3ACB8255B56" 
			similar="0000001A7774" />
	</gps>
	<city
		has="D3C74C7A7AB4"
		similar="0000001D3C74" />
</location>
\end{lstlisting}
\caption{Hash Structure Generated For Location Schema}
\label{fig:locationhashes}
\end{figure}


This collection is in fact built using lists, where each composite item contains both its hashes and a pointer to a list of its childrens' hashes.
This representation retains the overall structure of the schema, which will be used for nested searches. 
Figure \ref{fig:locationhashes} shows an XML representation of the hash collection which is generated for the schema in Figure \ref{fig:locationschema}.


% how to query RDC.
To construct a schema query for the RDC, each item in the constraint list is looked up in the hash collection and the appropriate hash is added to the item, depending on whether the item states an exact or type match. 
Figure \ref{fig:nested_query_hash} shows the query in Figure \ref{fig:nested_query} with these hashes. 
This list can then be sent to the RDC, which uses it to search for matching components. 

% nested query hashes representation
\begin{figure}
\begin{lstlisting}
<constraint>
	<name>"gps"</name>
	<exact>false</exact>
	<hash>"931DEF188280"</hash>
	<children>
		<constraint>
			<name>"longitude"</name>
			<exact>true</exact>
			<hash>"BE25D0A889CA"</hash>
			<children></children>
		</constraint>
	</children>
</constraint>
\end{lstlisting}
\caption{Nested Query Representation with Hashes}
\label{fig:nested_query_hash}
\end{figure}

% how RDC searches.
The RDC has the schema of each endpoint on each registered component, therefore has the hash collection of each endpoint. 
This hash collection is only computed once. 
The RDC returns all matching endpoints, not just the first match, so compares the query with each endpoint schema. 

\begin{algorithm}
\algnotext{EndIf}
\begin{algorithmic}[1]
\Function{CompareSchema}{$constraints$, $schema$}
	\ForAll{$constraint \in constraints$}
		\If{$constraint.hash = schema.hash(constraint.exact)$}
			\If{$constraint.exact$} \label{alg:line:constraint_exact}
				\Continue \Comment match, check next constraint
			\ElsIf{$constraint.children = null$}
				\Continue \Comment match, check next constraint
			\Else \label{alg:line:nested_start}
				\ForAll{$nested \in constraint.children$}
					\State $match\gets false$
					\ForAll{$child \in schema.children$}
						\State $match\gets \Call{CompareSchema}{nested, child}$
						\If{$match$}
							\Continue \Comment match, check next nested constraint
						\EndIf
					\EndFor
					\If{$match \neq true$}
						\Break \Comment no match, skip other nested constraints
					\EndIf
				\EndFor
				\If{$match$}
					\Continue \Comment match, check next constraint
				\EndIf
			\EndIf \label{alg:line:nested_end}
		\Else
			\State $match\gets false$
			\ForAll{$child \in schema.children$}
				\State $match\gets \Call{CompareSchema}{constraint, child}$
				\If{$match$}
					\Continue \Comment match, check next constraint
				\EndIf
			\EndFor
			\If{$match \neq true$}
				\State \Return $false$
			\EndIf
		\EndIf
	\EndFor
	\State \Return $true$
\EndFunction 
\end{algorithmic}
\caption{Pseudocode to Compare Schemas}
\label{alg:compare_schema}
\end{algorithm}

Algorithm \ref{alg:compare_schema} shows how the RDC checks whether a schema satisfies a set of constraints. 
Ignoring lines \ref{alg:line:nested_start}--\ref{alg:line:nested_end}, which check child constraints, the algorithm is simply a depth-first search. 
Each constraint is checked against the schema. 
If the constraint hash matches the appropriate schema hash, then the next constraint is checked, or the algorithm returns {\tt true} if there are no more constraints. 
If the hashes do not match, then the algorithm recursively calls itself using each of the schema's children as the new schema to compare against, until a match is found. 
If no match is found, then the algorithm returns {\tt false}. 
Lines \ref{alg:line:nested_start}--\ref{alg:line:nested_end} match nested constraints very similarly to the entire algorithm as described above. 
However, because these constraints specify fields which must be nested, then cannot be compared to the current item being matched. 
Therefore, the comparison must be conducted against each of the schema's children. 
Once a match is found the next constraint is tested, and if all nested constraints are matched, then the parent constraint is considered matched. 
Line \ref{alg:line:constraint_exact} checks whether a constraint is exact before its children are considered, since if it is an exact match, the children must match by definition. 

% optimisations to search
Queries are automatically optimised by components before being sent to the RDC, to increase performance of the algorithm. This allows the RDC to process the query as quickly as possible, preventing it from becoming a bottleneck in the system. 
Any duplicate constraints are first removed, for obvious reasons. 
Queries are then reordered to place either exact constraints first, constraints on composite items first, or a combination of these. 
This orders constraints such that those which are more difficult to match are checked first, causing the algorithm to fail earlier rather than later, thereby wasting less time. 
The result of these optimisations is explored in the Evaluation. 

%TODO: other ways the search could be optimised - hash table to linked lists, search tree. Potential performance increases + how results might vary.


% construct lookup
Once the RDC returns a list of matches, the component connects to one of these matches. 
During the connection setup, some schema negotiation must occur, so that the components can communicate coherently. 
This work occurs almost entirely on the component which initiated the query, so as not to hinder the target peer. 
The component first requests the target's schema. 
Upon receiving this schema, the component checks it against the original query, building a lookup table, while also ensuring that it does in fact match, and the RDC has not been replaced by some malicious process. 
This lookup table consists of a section containing names from the target schema, indexed by names from this schema, and a section containing names from this schema indexed by the target schema, and is used to repackage messages according to this component's schema. 

\begin{figure}
\begin{lstlisting}
@place
{
	coordinates
	{
		position
		{
			dbl longitude
			dbl latitude
			int height
		}
	}
	txt country
}
\end{lstlisting}
\caption{Complex Location Schema}
\label{fig:complexlocationschema}
\end{figure}

\begin{table}[b]
\centering

\begin{tabular}{c c}
\hline\hline
Current & Target \\
\hline
{\tt longitude} & {\tt longitude} \\
{\tt gps} & {\tt position} \\
{\tt latitude} & {\tt latitude} \\
{\tt altitude} & {\tt height} \\

\hline
\end{tabular}

\caption{Lookup Table}
\label{tab:lookup_table}
\end{table}
 
These entries are added every time a successful match occurs in Algorithm \ref{alg:compare_schema}. 
Additionally, entries can be inserted for all descendants of a successful match, since the structure of the schemas must be the same. 
An endpoint using the schema in Figure \ref{fig:locationschema} would create the lookup table in Table \ref{tab:lookup_table} for the schema in Figure \ref{fig:complexlocationschema} based on the query {\tt +Sgps[+Hlongitude]}.

Once this basic lookup table has been created, corresponding fields within the two schemas can be located, which allows a correspondence between outer structures to be inferred, as shown in Algorithm \ref{alg:construct_lookup_outer}. 
The first loop works upwards from a corresponding field in the schema and target schema. 
Given that these fields match, their ancestors must also match. 
This loop will end once one of the schemas has no parent, adding ({\tt location}, {\tt coordinates}) to the lookup table in this case. 
Only one schema will have remaining ancestors, however if the top level of both schemas has been reached, no further action is taken. 
If the target schema has remaining ancestors, these represent layers which surround the message the component will use. 
These are stored so that a message can be extracted from within these layers. 
Here, {\tt place} is the only extra layer and {\tt place/coordinates} is the root of the message the component wants. 
If the component schema has remaining ancestors, these represent layers which are missing from the message. 
These are stored so that the message can be wrapped in them. 

\begin{algorithm}
\algnewcommand\And{\textbf{ and }}
\algnotext{EndWhile}
\begin{algorithmic}[1]
\While{$current \neq null \And target \neq null$}
	\State \Call{InsertLookup}{$current.name$, $target.name$}
	\State $current\gets current.parent$
	\State $target\gets target.parent$
\EndWhile
\While{$target \neq null$}
	\State \Call{AddExtra}{$target$}
	\State $target\gets target.parent$
\EndWhile
\While{$current \neq null$}
	\State \Call{AddMissing}{$current$}
	\State $current\gets current.parent$
\EndWhile
\end{algorithmic}
\caption{Lookup Table Inference for Outer Structures}
\label{alg:construct_lookup_outer}
\end{algorithm} 

% updating subscriptions
After the lookup table has been created, any predefined subscription filter can be applied to the peer. 
These filters are specified in terms of field names, for example the filter {\tt location/gps/latitude > 0.0} specifies the northern hemisphere. 
If the component schema has additional ancestors, then these are removed from the filter, while if the target schema has additional ancestors, the filter is prepended with these, replacing the top level node to make the filter {\tt place/coordinates/gps/latitude > 0.0}.
Other field names are then replaced with their corresponding entry in the lookup table, creating a filter which when applied to the target schema, has the expected effect, namely {\tt place/coordinates/position/latitude > 0.0}.
Any filters on fields which do not exist in the target schema are applied as specified, so developers must ensure that fields which they intend to filter on are included within their schema query. 
This converted filter will be applied in the final stage of the connection setup, and of course can be updated at any later time. 

\begin{algorithm}
\begin{algorithmic}[1]
\Function{Repack}{$message$}
	\If{message has additional layers}
		\State \Call{UnwrapMessage}{$message$}
	\EndIf
	
	\State \Call{RepackageMessage}{$message$}
	
	\If{message is missing layers}
		\State \Call{WrapMessage}{$message$}
	\EndIf
\EndFunction 
\end{algorithmic}
\caption{Pseudocode to Repackage Message}
\label{alg:repack_message_overall}
\end{algorithm}

Finally, this lookup table will be used to repackage each message. 
Algorithm \ref{alg:repack_message_overall} shows how each message is repackaged. 
Suppose the component receives the message in Figure \ref{fig:locationmessage}. 
The first stage is to extract the desired message from any additional layers, in this case {\tt place/coordinates}. 
This message is then repackaged under the names defined by this component's schema using Algorithm \ref{alg:repack_message}. 
\begin{algorithm}
\algnotext{EndIf}
\begin{algorithmic}[1]
\Function{RepackageMessage}{$message$}
	\If{$message.name \in lookup\_table$} \label{alg:line:lookup_name_start}
		\State $name\gets \Call{Lookup}{message.name}$
	\Else
		\State \Return $null$
	\EndIf \label{alg:line:lookup_name_end}
	
	\If{$message.children = null$} \label{alg:line:check_children_start}
		\State \Return \Call{Pack}{$message, name$}
	\Else
		\State $composite\gets \Call{CreateStructure}{name}$
	\EndIf \label{alg:line:check_children_end}
		
	\ForAll{$child \in message.children$} \label{alg:line:repack_children_start}
		\State \Call{AddMissingChildren}{$composite$}
		
		\State $node \gets \Call{RepackageMessage}{child}$
		\If{$node \neq null$}
			\State \Call{AddNode}{$composite, node$}
		\EndIf
	\EndFor \label{alg:line:repack_children_end}
	
	\State \Call{AddMissingChildren}{$composite$}

	\State \Return $composite$
	
\EndFunction
\end{algorithmic}
\caption{Pseudocode to Repackage Message}
\label{alg:repack_message}
\end{algorithm}
Lines \ref{alg:line:lookup_name_start}--\ref{alg:line:lookup_name_end} find the correct name from the lookup table, returning {\tt null} if it does not exist in the table. 
Lines \ref{alg:line:check_children_start}--\ref{alg:line:check_children_end} then either pack the message content under the new name, if it is a single field, or start a composite item otherwise. 
The loop spanning lines \ref{alg:line:repack_children_start}--\ref{alg:line:repack_children_end} then iterates over each item in the message. 
The name of each item can be used to check if any items are missing from the message, and add them between their siblings. 
The message is otherwise repackaged in the same way, and added to the composite item if it should appear. 
Any other siblings missing from the message can be added at the end. 
Finally, if the component schema had additional layers rather than the peer schema, the message would be wrapped in these missing layers. 
This repackages the message in Figure \ref{fig:locationmessage} to Figure \ref{fig:locationmessagerepack}, which can then be used by the component.  

% repackage messages
\begin{figure}
\begin{lstlisting}
<place>
	<coordinates>
		<position>
			<longitude>0.091732</longitude>
			<latitude>52.210891</latitude>
			<height>19</height>
		</position>
	</coordinates>
	<country>"UK"</country>
</place>
\end{lstlisting}
\caption{Example Location Message}
\label{fig:locationmessage}
\end{figure}

\begin{figure}
\begin{lstlisting}
<location>
	<gps>
		<longitude>0.091732</longitude>
		<latitude>52.210891</latitude>
		<altitude>19</altitude>
	</gps>
	<city>""</city>
</location>
\end{lstlisting}
\caption{Repackaged Location Message}
\label{fig:locationmessagerepack}
\end{figure}

\section{Android Port}

The Android NDK was used to compile SBUS to support Android, and embed the native machine code into applications. 
The documentation for the NDK offers little more than trivial examples, and few other resources exist, so this stage of the project was largely achieved through experimentation. 

A standalone toolchain from the NDK can be used to set the correct compilation variables when running {\tt ./configure}. 
In setting up the toolchain, the architecture of the device and the Android native API platform must be specified. 
ARM-based devices, and Android platform 9 where chosen here, since these corresponded to the available device. 
The standalone toolchain is still in beta which meant there were several problems compiling SBUS directly, typically due to missing libraries. 
Some SBUS code had to be rewritten to account for this, in many cases using {\tt \#ifdef} statements to substitute blocks of code specific to Android. 
The toolchain setup contained a bug\footnote{\url{https://code.google.com/p/android/issues/detail?id=35279}} which meant some include paths were missing, in particular {\tt <limits>} could not be referenced. 
The workaround for this was fixing the directory structure of the toolchain so that the paths were correct. 

The Android version of {\tt pwd.h} which defines {\tt struct passwd} contains no member {\tt pw\_gecos}. 
This contains the user's full name, which is used by SBUS to set the {\tt creator} field in a component's metadata, allowing components to query an RDC by creator. 
To remedy this, the creator was simply replaced by the string `Android'. 
In principle, the user's full name or username could be found in Java using the AccountManager class\footnote{\url{http://developer.android.com/reference/android/accounts/AccountManager.html}}, which could then be passed as a parameter to the component via JNI. 
This would allow the creator to be set correctly, however it was deemed unimportant to the project. 

% IP address.
One of the greatest problems was that {\tt <sys/sockio.h>} was missing from Android.
SBUS uses this as part of determining the IP address a component is running on, which is then used during connection setup. 
This meant that the entire method to determine the IP address had to be rewritten to work on Android. 
This was achieved by passing an {\tt ifconf} structure to the {\tt ioctl()} system call.
This populates the structure with a list of network interfaces and their IP addresses, which can then be searched for the appropriate interface to find the IP address. 
In this case, the {\tt eth0} interface for Wi-Fi was prioritised over the 3G interface.

% Android FS.
Another large issue was the Android filesystem. 
When creating a component, SBUS invokes a wrapper which is responsible for handling the low level messages between components, passing them to the library level as structured messages.
This typically resides at {\tt /usr/bin/sbuswrapper} which means it can be invoked via a call to {\tt sbuswrapper}, since the {\tt PATH} environment variable contains {\tt /usr/bin}. 
For the same functionality, the wrapper needed to be stored in a directory which is in the {\tt PATH} variable, which on Android, amounts to {\tt /system/bin}. 

The Android SDK provides the Android Debug Bridge ({\tt adb}) which can be used to push and pull files to and from a device. 
When using the Android emulator, {\tt /system} can be remounted as writable\footnote{using {\tt mount -o rw,remount -t yaffs2 /dev/block/mtd3 /system }}, allowing {\tt sbuswrapper} to be pushed to {\tt /system/bin/sbuswrapper}. 
However, on a device without root access, {\tt /system/bin} cannot be remounted in this way, thus cannot be written to. 
Therefore, some other writable directory was needed, and {\tt sbuswrapper} would have to be invoked using the full path. 

The SD card ({\tt /mnt/sdcard}) is writable so was considered, however is mounted with the option {\tt noexec}, which means that although a file can be pushed to it, it cannot be executed. 
The best approach to use was to include {\tt sbuswrapper} as an asset to an application, and copy it to the application's filespace, invoking it from this filespace for every application. 

Android uses a sandbox concept for security, where each application has access only to its own resources on the device. 
This is enforced by assigning each application a unique user ID, and setting the appropriate permissions. 
Since {\tt sbuswrapper} is approximately \SI{1}{MB}, compared to approximately \SI{30}{MB} available internal memory on the device, it was infeasible to include a copy with every application. 
Therefore, one copy was included with the Phone Manager, which then modified the permissions to make {\tt sbuswrapper} world executable, so that every application could use this instance. 
An additional directory was created and made world writable, where SBUS can write log files for all applications. 
If memory was not an issue on the device, each application could store its own copy of {\tt sbuswrapper} in its filespace, and log to its own filespace. 
The path to this filespace can be obtained using an Android library call, which could be used as a parameter to a JNI method to set the location of {\tt sbuswrapper}, avoiding hardcoding any paths. 

Although the device could have been rooted, allowing a much cleaner solution of storing {\tt sbuswrapper} at {\tt /system/bin/sbuswrapper}, it would have devalued the project, because it would only then work on rooted devices.

%TODO: write about closing sockets and all those FIN_WAIT2? Nothing actually changed now, but it was a pain.

% JNI.
The Android NDK was finally used in conjunction with Java Native Interface (JNI) to allow applications to call SBUS functions via Java. 
This involved writing Java classes for components, endpoints and messages, as well as some helper classes. 
The SBUS library is linked via an {\tt Android.mk} file, which {\tt ndk-build} then uses to copy the shared library into the application's root project directory. 
The application can then be built to generate a package file including SBUS which can run on an Android device. 
 
 % overview of Android architecture
\begin{figure}[h]
%set the image x size to the width of the page
\epsfxsize=8cm
\centerline{\epsfbox{figs/android_stack.eps}}
\caption[Android System Architecture]{A brief overview of the Android system architecture, based on the image at \url{http://developer.android.com/images/system-architecture.jpg}}
\label{fig:android_stack}
\end{figure}

% how Android works
Figure \ref{fig:android_stack} gives an overview of how an application works. 
Applications are built using the Application Framework. 
Every Android application runs in its own process, with its own instance of the Dalvik virtual machine\footnote{\url{http://developer.android.com/about/versions/index.html\#os_architecture}}, which executes Java bytecode. 
When an SBUS call is made, Dalvik invokes a call to the native JNI code, passing in a {\tt JNIEnv} pointer containing an interface to the Dalvik VM, as well as any other parameters. 
The JNI methods can then invoke their respective SBUS methods. 

Each Java object has a one-to-one correspondence with a native object. 
Native methods which create an object return a pointer to that object, cast as an integer. 
The pointer is passed to the constructor of the Java object representing the native object, to be stored in the object. 
When a method is invoked on the Java object, the pointer is passed into the native method, which then casts it to a pointer to the native object, to operate on the object. 
The Java classes hide this complexity from the user, simply providing methods with parameters and return values in terms of these other Java classes.

These classes are exported as a JAR file, which applications can then simply import to use the Java SBUS library.

\section{Phone Manager}

The Phone Manager is an Android application which is responsible for detecting changes in context and reconfiguring other applications' connections. 

% detecting context
The Phone Manager uses the AIRS application\footnote{\url{https://play.google.com/store/apps/details?id=com.airs}} and Android system intents\footnote{\url{http://developer.android.com/reference/android/content/Intent.html}} to detect changes in context. 
These intents are broadcast across the system upon specific events, and by implementing a receiver, these intents can be captured and an action can be taken. 
In particular, an intent with the action {\tt NETWORK\_STATE\_CHANGED\_ACTION} is broadcast upon a Wi-Fi change.
This allows the Phone Manager to register a receiver for these intents, allowing it to reconfigure connections after a change in network. 

% AIRS
An application has been created to act as a AIRS-SBUS gateway between AIRS and the Phone Manager. 
This gateway acts as a remote server for sensor readings from AIRS, to which AIRS sends all sensor readings. 
Once AIRS starts and connects to a remove server, sensors must be subscribed to before AIRS will output any readings. 
AIRS connects to the gateway, and the Phone Manager can then connect to the gateway via an SBUS endpoint, and subscribe to sensors through another ``control endpoint'' on the gateway. 
Upon receiving a subscription, the gateway relays this subscription to AIRS in the appropriate format. 
AIRS can then send sensor readings to the gateway, which will convert them to SBUS events and send them to the Phone Manager. 
This allows the Phone Manager to use any sensor on the device which is exposed by AIRS, yet ensures that the Phone Manager only receives readings from sensors that it is interested in. 
 
 % AIRS-SBUS gateway
This gateway is based on Dirk Trossen's AIRS code\footnote{\url{https://github.com/dirktrossen/AIRS}}. 
AIRS exposes upwards of 50 sensors, which is being continually expanded, making it infeasible to predefine an endpoint for each sensor. 
This is handled by dynamically creating an endpoint for each sensor whenever the first reading from that sensor is received, meaning that endpoints will only ever be created as needed. 
The gateway presents the suer with a list of sensors, which is populated from a message AIRS announcing the available sensors upon connecting to the remote server. 
When the user selects a sensor, that sensor is subscribed to, and the endpoint will be created once a reading is received from that sensor. 
Additionally, the endpoint is automatically mapped to the Phone Manager so can start publishing events to it immediately. 
Each sensor runs in a different thread, so these endpoints are maintained in a thread-safe repository. 
This repository is searched upon receiving a message, returning the endpoint if it already exists, ensuring that multiple endpoints are not defined for the same sensor. 

% composite event detector
Conceptually this AIRS-SBUS gateway could act as a composite event detector. 
By collating multiple sensor readings, a context change could be defined by multiple different readings satisying some conditions, or by multiple occurrences of an event.
Upon detecting a context change, the gateway would then publish a single event to the Phone Manager. 
The Phone Manager can respond to any kind of event, so already provides the infrastructure required to support this.

% registering components + setting policies
In order to have their connections automatically reconfigured, applications must register with the Phone Manager. 
This registration occurs in exactly the same way as with an RDC; the Phone Manager is essentially a lightweight RDC which does not offer search functionality. 
Once an application has registered, it can set {\sl map policies}, which determine how its connection will be reconfigured. 
A policy contains the application endpoint name, a peer endpoint name, and a peer address in the form of an IP address or an RDC query which will be resolved to an address. 
The Phone Manager will apply these policies exactly as specified, either instructing an application endpoint to map to another endpoint at some IP address, or to first resolve an RDC query, then map to the endpoint. 

% RDC updates 
Some context changes may mean that the available RDCs change, for example joining or leaving a network. 
The Phone Manager informs applications of RDC changes on every context change, publishing an event containing the RDC address, and whether it has became available or unavailable. 
Currently an RDC is only assumed to become unavailable if the context change is leaving a Wi-Fi network. 
The RDC change event is published to each registered application via a builtin endpoint. 
Upon receiving the event, applications can either automatically accept the change, registering with or deregistering from the RDC address, or perform a callback method, perhaps prompting the user to make a decision. 

% where to get RDC address from
This RDC address is currently set by the user in the Phone Manager, however many other ways exist which could be used to find an RDC in the environment. 
Prompting the user may be sufficient on a home network, and additionally this allows the user to choose not to trust an IP address. 
Alternatively, the address could be coded in a QR code or RFID tag. 
The user could scan one of these, for example when making a purchase in a coffee shop, and the Phone Manager would automatically send an RDC update based on the encoded address. 
This approach retains some level of privacy and security because the user explicitly chooses to scan the QR code or RFID tag. 
An even more automated approach might be to include the RDC address as part of DHCP configuration when connecting to a Wi-Fi network, or to broadcast the address on the network. 
All of these methods must of course be considered from security point of view. 

% apply map policies
After the Phone Manager sends an RDC update to registered applications, it can apply applications' map policies. 
To do this, it simply issues map events to application endpoints, containing the peer address and peer endpoint name specified in their policies. 
If the peer address specifies an IP address, then the endpoint can map directly to the peer endpoint. 
If the peer address is an RDC query, then the application must first resolve the query, which may be done against a new RDC which has been specified in the update. 
This allows the query to be resolved in the new environment, allowing endpoints to map to peers within that environment. 


\section{Issues}
% remaining problems
The system currently faces issues when context is rapidly changing, for example, moving in and out of range of a Wi-Fi network. 
The network connection drops causing the Phone Manager to reconfigure connections, which may disconnect applications from their peers.
However, the network connection is almost immediately re-established causing the Phone Manager to reconfigure connections again, possibly connecting applications to these same peers. 
If this reconnection occurs before disconnection has completed, an application believes itself to still be connected to the peer, thus will not create a new connection.

A solution to this problem might be to implement conditions which must be satisfied by an event before reconfiguration occurs. 
In a Wi-Fi network, the network signal may be required to be greater than a given threshold, meaning reconfiguration would not occur until the user moves firmly into the network. 
Alternatively, a composite event detector could trigger events after multiple occurrences of a single event, so that reconfiguration is not performed as frequently. 
For example, a single event could be triggered after three consecutive Wi-Fi events have occurred with increasing signal strength. 

% qualify names if multiple fields in sensor have same name
Components specify RDC schema queries in terms of field names of their own schema.
While this is sufficient if there are no repeated fields names within the schema, it would fail on a schema such as Figure \ref{fig:repeatednameschema} due to {\tt sensor} being repeated. 
A solution to this would be to force field names to be fully qualified in a query. 
A query using {\tt sensor} would refer to the entire structure, while {\tt sensor/sensor} would refer to the text field.

\begin{figure}[h]
\begin{lstlisting}
@sensor
{
	txt sensor
	int value
}
\end{lstlisting}
\caption[Ambiguously Named Schema]{A schema with a structure named {\tt sensor} and a text field also named {\tt sensor}. A constraint specified on {\tt sensor} would be ambiguous without fully qualified names}
\label{fig:repeatednameschema}
\end{figure}


\cleardoublepage


% ~ Eval + Conclusion ~ 2000 words
\chapter{Evaluation}

The project set out to investigate how applications could automatically connect to different peers upon a change in context, where these peers may not necessarily present data in the format expected. 
The project has been successful in this respect, implementing a system which gives applications these capabilities.  
Sensor readings from the AIRS application\footnote{\url{https://play.google.com/store/apps/details?id=com.airs}} and Android intents are used to detect a change in context. 
Once a change is detected, applications will be connected to peers which satisfy the constraints they have specified and messages will automatically be converted to applications' expected format, allowing operation of the application to continue without interruption. 
This chapter examines the performance of the system. 

\section{System Overview}

% overview of system image
\begin{figure}[t]
%set the image x size to the width of the page
\epsfxsize=\hsize
\centerline{\epsfbox{figs/overview.eps}}
\caption[System Overview]{An overview of the system, showing interactions between different parts upon a context change}
\label{fig:system_overview}
\end{figure}

Figure \ref{fig:system_overview} shows the interactions that occur within the system, detailing how a transport application which uses the coach schema in Figure \ref{fig:coach-schema} might have its connection reconfigured upon a change in context.

\begin{enumerate}

\item After the transport application has registered with the Phone Manager, it can send policies to the Phone Manager which specify how its endpoints should be reconfigured upon a change in context. For instance, the application may wish to be connected to any components which offer a structure matching the type of {\tt coach}. In another case, it may simply give an IP address it should be connected to.

\item The AIRS-SBUS gateway acts as a server for sensor readings from AIRS, which then emits the readings as SBUS events, which are consumed by the Phone Manager. The Phone Manager also receives intents from Android, signalling a change in Wi-Fi network.

\item The Phone Manager processes these events to determine whether a change in context has occurred. Once a change happens, the Phone Manager will inform the transport application about the change by sending it details of whether an RDC should be added or removed. 

\item The transport application receives this message about the RDC change and will either automatically accept this change, registering or deregistering with the RDC, or perform some callback method, perhaps prompting the user to make a decision. The transport application may have chosen to automatically accept the change, but display a message to the user to indicate that the context has changed.

\item The Phone Manager applies all of the policies within its policy table, sending messages to application endpoints, instructing them to connect to different components. The transport application will be sent whatever parameters it set in its policy.

\item If the transport application specified an IP address in its policy, the endpoint will receive this IP address and can connect directly to it to start receiving messages. However, if the policy specified that a component must contain a structure matching the type of {\tt coach}, the endpoint will query the RDC with this constraint to find matching components.

\item The RDC searches components registered with it for those matching the constraints. In this case, it might find only a component sending coach data, while in other cases it may find components offering train data. In both cases, these components contain a structure matching the type of {\tt coach}, so the RDC returns the IP addresses to the application.

\item The application endpoint then connects to one of the matching components, allowing the user to continue receiving fresh travel data without any interaction needed.

\end{enumerate}

% other ways to do context
AWARE\footnote{\url{http://www.awareframework.com/home/}} is a new ``Android framework dedicated to instrument, infer, log and share mobile context information for application developers'' which could be used in place of AIRS to detect a change in context.
AWARE broadcasts intents to notify applications of the current context, and these broadcasts can be captured by Android BroadcastReceivers\footnote{\url{http://developer.android.com/reference/android/content/BroadcastReceiver.html}} in the same way that other intents are already captured. 
This allows the Phone Manager, or indeed any application, to implement broadcast receivers to be notified of changes in context, rather than using AIRS and the AIRS-SBUS gateway. 
Some applications might choose to implement broadcast receivers themselves, in order to use this framework in place of the Phone Manager to detect context changes.
However, the Phone Manager would still be useful because it would provide this service for any application to use, mapping them as per their policies and reducing complexity in the applications themselves.

% other messaging middlewares
Mascolo et al. give an overview of other middlewares for mobile computing \cite{mascolo2002mobile} which the project might have used. 
SBUS is in line with many of their guidelines for a middleware, thus proves an ideal candidate for the project. 
Direct communication between components means that the middleware supports devices connecting to the network for short periods of time just as well as those which are permanently part of the network. 
Components are written in C++ and already run on most machines, and can now run on any Android phone too, allowing for heterogeneity in an environment. 
Communication is not only independent of the device and platform, but also the network connectivity. 
Components communicate using TCP, which means that the middleware will work over Wi-Fi, 3G, Bluetooth, and any other protocol supporting TCP. 
Resource discovery is already implemented through the RDCs, and has been extended to support the needs of mobile computing. 
Finally, SBUS can use SSL to encrypt data, allowing applications to communicate securely if they so desire. 


% table of example policies
\begin{table}
\centering

\begin{tabular}{c c c c}
\hline\hline
Component & Endpoint & Remote Address & Remote Endpoint \\
\hline

Healthcare & HeartRate & 128.232.0.20:44444 & HeartRate \\
Transport & Departures & +NStagecoach & Coach \\
Transport & Departures & +Scoach & NULL \\

\hline
\end{tabular}

\caption{Phone Manager Example Policies}
\label{tab:example_policies}
\end{table}

% about example policies
Table \ref{tab:example_policies} shows example policies which components might set with the Phone Manager. 
Both the local component and local endpoint are implicitly set to whichever endpoint sent the policy. 
The remote address may be an IP address or a query to be resolved by an RDC. 
An IP address may be useful when the application should always be mapped to the same IP address. 
In the first example, a healthcare system has registered to have heart rate readings mapped directly to an endpoint at a known IP address, perhaps a relative or emergency response team. 
However the real flexibility of the system lies in the queries. 
These will be resolved by some RDC, ensuring components are mapped only to other components satisfying their queries. 
In the second row, the transport application sets a policy for any component named ``Stagecoach'' which has an endpoint named ``Coach'', while in the third row it sets a policy for any endpoint which has a structure matching the type of {\tt coach}. 

% event-condition-action through specifying AIRS sensor + condition
The system is in line with an event-condition-action model, where events are the sensor readings and actions are mapping applications according to their policies. 
The system could easily be extended to allow applications to specify which sensors to use as events, and conditions those events should fulfill for the Phone Manager to apply a mapping policy. 
% table of example policies with events and conditions
\begin{table}
\centering

\begin{tabular}{c c c c}
\hline\hline
Event & Condition & Component & Remote Address \\
\hline

Accelerometer & \begin{math} val \ge50 \end{math} & Healthcare & 128.232.0.20:44444 \\
Time & \begin{math} 12 \le val.hour \le 14 \end{math} & Adverts & +IRestaurant \\
Wi-Fi & \begin{math} val = true \end{math} & Transport & +NStagecoach \\
GPS & \begin{math} location(val) = France \end{math} & Transport & +Scoach \\

\hline
\end{tabular}

\caption{Phone Manager Example Policies using Event-Condition-Action Model}
\label{tab:event_condition_action}
\end{table}
% end table
As shown in Table \ref{tab:event_condition_action}, applications would not only specify the action they wish to take through their map constraints, but also the events and conditions in which they wish to take that action.
The healthcare system could specify that it only wishes for heart rate data to be connected to the IP address if a high accelerometer value was measured, because the reading may indicate that a fall has occurred. 
The transport application can set different policies for different events. If a Wi-Fi connection is established, the application wants to connect specifically to Stagecoach components, however if a GPS event indicates that the user were in France, the application is more lenient and willing to accept any component offering a structure matching the type of {\tt coach}. 
The application may display adverts, where policies could be used to determine the source of the adverts, perhaps from nearby restaurants around lunchtime. 

% user policies
This could further be used to allow the user to set their own policies for applications. 
The application developer cannot possibly account for every scenario, however by allowing the user to set their own policies, through which they are specifying context changes they are interested in, the system becomes fully customisable and personal. 
The user of the transport application might not want to wait for a bus when it is raining. 
Therefore, they could create a policy using a weather event with the condition ``rain''. 
The action might be to connect the application to the IP address of a local taxi company, which provides information about when the next taxi will be available using the schema in Figure \ref{fig:taxi-schema}.

%TODO: UI for user to set policy (system can also set policy).

%TODO: Screenshots of original message + repackaged?

%TODO: How long to go through policy list (all the if statements)

%TODO: How long to tell a component about context change - long list, might take a while for last one - prioritise?

\section{System Performance}

This section examines the performance of the system, specifically aspects related to the schema conversion part of the system. 
The schema conversion plays an important role in mobility because it allows applications to function despite some level of uncertainty, which in turn makes them more adaptable to different environments. 
This schema conversion can only be useful if it can be done in a reasonable time though, since the whole idea behind the system is to provide users with meaningful, {\sl real time} data. 
These tests show the kind of performance which can be expected from the system. 

% rdc search optimisation
\begin{figure}[t]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/rdc_search_optimisations.eps}}
\caption{RDC Search Optimisations}
\label{fig:rdc_search_optimisations}
\end{figure}

Figure \ref{fig:rdc_search_optimisations} shows the time taken for an RDC to search through all registered components to find those matching schema constraints, for different numbers of registered components and with different optimisations applied. 
In a real environment, RDCs are likely to have many components registered, so one search must not impact the performance of any other component too greatly.
As expected, the time taken to search increases linearly with the number of components registered. 
The structures first optimisation narrowly beats the exact first and exact structures optimisations, all of which peform more efficiently than searching without optimisation. 
These optimisations play a greater role as the number of components increases, becoming more important in order to prevent a bottleneck in the system. 
However, even in the worst case the RDC can perform the search with 100 other components registered in just over 40 milliseconds, so should not pose a problem given expected usage.

% construct lookup
\begin{figure}[t]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/construct_lookup.eps}}
\caption[Construct Lookup Times]{Time taken to construct a lookup table between schemas of different sized producers and schemas of different sized consumers}
\label{fig:construct_lookup}
\end{figure}

Figure \ref{fig:construct_lookup} shows the time taken to construct a lookup table between schemas of different sized producers and schemas of different sized consumers. 
The lookup table is created before any messages can be exchanged between peers with different schemas. 
If this were to take too long, important messages be missed, or messages could become stale, no longer containing correct data.
The general increase in time from left to right shows that more time is taken to construct a lookup table as the size of the producer schema increases, as expected, since there are more fields. 
The greater values in the upper right half of the graph show a disparity with the smaller values in the lower left half of the graph. 
This shows that the time taken to construct a lookup table is greater when the producer's schema is larger than the consumer's schema, corresponding to searching in the larger producer schema for the smaller consumer schema to be matched, as opposed to identifying the missing outer structures if the consumer schema were larger.
In either case, the lookup table is constructed once per connection taking only microseconds, so is not a significant overhead. 

% repack message
\begin{figure}[t]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/repack_message.eps}}
\caption[Repackage Message]{Time taken to repackage a message between schemas of different sized producers and schemas of different sized consumers}
\label{fig:repack_message}
\end{figure}

Figure \ref{fig:repack_message} shows the time taken to repackage a message between schemas of different sized producers and schemas of different sized consumers. 
This repackaging happens once per message, thus will cause a delay on every single message received. 
Should this delay be too large, then messages may contain stale data by the time they have been repackaged. 
The general increase in time from top to bottom shows that more time is taken when the consumer schema is larger, as expected because more fields are being repackaged under the appropriate names. 
As with constructing the lookup table, messages are repackaged within microseconds, which is an insignificant overhead. 
In general, messages should still be relevant after the time taken for repackaging, for any realistic use of the system.

Both Figure \ref{fig:construct_lookup} and Figure \ref{fig:repack_message} use a schema constraint which matches the type of the smallest schema, to ensure that a match is made in every case. 

\section{Android Port}

This section evaluates the performance of using SBUS on a phone in comparison to using SBUS on a laptop, investigating whether any significant gains or losses occur as a result of the port.

% table showing map times
\begin{table}[h]
\centering

\begin{tabular}{c c c c}
\hline\hline

Consumer & Producer & Mean (ms) & Standard Deviation (ms) \\
\hline

Laptop	&	Laptop	& 7.979 	& 9.787 \\
Laptop	&	Phone	& 20.824 	& 20.064 \\
Phone	&	Laptop	& 21.834	& 15.705 \\
Phone	& 	Phone	& 10.933	& 2.810 \\
\hline
\end{tabular}

\caption{Connection Times Between Components on Different Devices}
\label{tab:map_times}
\end{table}

% figure showing map times
\begin{figure}[t]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/map.eps}}
\caption{Connection Times Between Components on Different Devices}
\label{fig:map_times}
\end{figure}

Table \ref{tab:map_times} and Figure \ref{fig:map_times} show mean times and standard deviations for consumers to connect to producers, across phones and laptops.
Each consumer first queries an RDC on a remote machine by the producer name, to find the appropriate producer to connect to. 
Once the consumer receives the IP of the producer from the RDC, it then establishes the connection with it, hence two connections occur within each mapping. 
No schema conversion occurs as part of this map. 
These figures show that a greater time is required when the producer and consumer are on different devices than when they are on the same device. 
This bears no surprise that local connections are faster than network connections. 
The mean time taken is slightly greater when both the producer and consumer are on the phone rather than both on the laptop, though the standard deviation is less. 
Overall, an application written in Java running a phone would see very similar network delays to that of a program written in C++ running on any other machine.

% figure showing jni times
\begin{figure}[t]
\epsfxsize=\hsize
\centerline{\epsfbox{figs/jni.eps}}
\caption{Function Call via JNI Timing Breakdown}
\label{fig:jni_times}
\end{figure}

Figure \ref{fig:jni_times} shows the breakdown of time taken to perform an SBUS function call via Java compared to C++. 
In this case, the function call is to emit a message. 
This shows that the function call in C++ takes slightly over one millisecond, while using Java to call the native method via JNI takes almost six milliseconds, almost a five millisecond delay. 
Each call to an SBUS function via Java will incur some delay, meaning that applications written in Java for a phone will never operate as quickly as those written in C++.
Application developers can expect this delay, which is likely to show less variance than any network delay, and can build their applications to take account of it. 
Provided an application is calling SBUS functions less frequently than this delay, such as a emitting a message less than once every ten milliseconds, it is unlikely to cause any problems.

%TODO: JNI times for other method calls?

%TODO: Stress test? How many components on Android can communicate?

%TODO: Example code for Android SBUS component?

%TODO: Screenshots?

\cleardoublepage

\chapter{Conclusion}

% summarise
The project has created a context-aware system which enables developers to write applications without prior knowledge of the location of the peers which the application will use, or the format that these peers will present data in, decoupling applications from their specific peers. 
This supports the goals of pervasive computing, achieving scalability by allowing applications to use any relevant peer, and allowing applications to adapt to use heterogeneous content, all while being invisible to both the user and developer.

An application monitors the environment for events which the system or user believe to represent a change in context.
Upon detection of an event, this application applies other applications' reconfiguration policies to connect them to different peers. 
Through these policies, applications can control which peers they should be connected to, without requiring the applications to find the peers themselves.
Policies may be simple, such as the name of a peer, or more complex, specifying how fields in a peer's schema must match their own. 

The system reformats data so that applications will only ever be presented with data conforming to their own format, removing uncertainty about the data representation. 
Furthermore, applications may specify subscription filters in terms of their own data format, placing restrictions on the data they will be sent, without requiring knowledge of peers' data format.
When reconfiguring a connection, the system translates these filters to match the peer's data format and applies them to the peer, filtering data as dictated by the application.

%\section{Changes}
%TODO: changes

\section{Future Work}
% other stuff that could be done

% alternate events for change in context - AIRS
Applications cannot currently specify {\sl which} changes in context they are interested in, so {\sl all} policies are triggered upon detecting a change.
The system could easily be extended to allow applications to include a type of context change in a policy, allowing them to specify different policies for different context changes. 
Upon receiving an event, each policy would only be applied if the event satisfies the conditions of the policy. 
This would allow applications to specify policies for simple events, such as a Wi-Fi change event, or more complex events, such as a high accelerometer reading followed by no movement to represent a fall. 

As well as applications specifying policies, users could also specify custom policies for applications. 
This moves the system further towards the pervasive vision. 
Applications would take sensible actions for different changes in context, however the user could refine how applications should react to changes in context, causing them to behave in a way more suited to the user's needs.

The user may be given the ability to specify which types of events can be used by the system. 
This offers some privacy to the user by allowing them to block applications from using some part of their context, for example blocking the system from using accelerometer to detect a change in context.

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
% nocite means add all references even if not cited.
\nocite{*}
\bibliography{refs}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\begin{appendix}

\chapter{Example Schemas}

\input{schemas}

\cleardoublepage

% should this be an appendix?
\chapter{Project Proposal}

\input{propbody}

\end{appendix}

\end{document}
